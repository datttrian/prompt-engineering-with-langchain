{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":31084,"status":"ok","timestamp":1706678778257,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"BZSHoJJhFP4_"},"outputs":[],"source":["%%capture\n","!pip install langchain==0.1.4 openai==1.10.0 langchain-openai datasets faiss-gpu sentence_transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6611,"status":"ok","timestamp":1706679000028,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"ROv0ygONORNx","outputId":"dd015dc1-952d-42d5-8783-9770243f0327"},"outputs":[],"source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":316,"status":"ok","timestamp":1706679002217,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"sq7OvPt1_2XX"},"outputs":[],"source":["from langchain.globals import set_verbose\n","\n","set_verbose(True)"]},{"cell_type":"markdown","metadata":{"id":"zEAWUvegOw27"},"source":["#ðŸ”— **Origins of CoT Prompting**\n","\n","- CoT was introduced by Wei et al. in their 2022 paper, [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://browse.arxiv.org/pdf/2201.11903.pdf).\n","\n","- It's a response to the need for better reasoning in large language models.\n","\n","ðŸ’¡ **What is CoT Prompting?**\n","- CoT stands for Chain of Thought Prompting.\n","\n","- It's about guiding language models through a step-by-step reasoning process.\n","\n","- The model shows its reasoning, forming a \"chain\" of thoughts, rather than just giving an answer.\n","\n","- Mimics human-like thought processes for complex tasks.\n","\n","\n","### ðŸŽ“ **Standard vs. CoT Prompting Example**\n","\n","<figure>\n","  <img src=\"https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1696369825-cot-example.png&w=3840&q=75\" alt=\"Image Description\" style=\"width:100%\">\n","  <figcaption>Examples of âŸ¨input, chain of thought, outputâŸ© triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted.</figcaption>\n","  <a href=\"https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1696369825-cot-example.png&w=3840&q=75\">Image Source</a>\n","</figure>\n","\n","- Standard: Direct answer with no reasoning (e.g., \"11\").\n","\n","- CoT: Detailed reasoning steps (e.g., \"Roger started with 5, added 6 from 2 cans, so 5 + 6 = 11\").\n","\n","### ðŸš€ **Usage of CoT Prompting**\n","\n","1. **Enhanced Reasoning**: Breaks down complex problems into simpler steps.\n","\n","2. **Combination with Few-Shot Prompting**: Uses examples to guide model responses, great for complex tasks.\n","\n","3. **Interpretability**: Offers a clear view of the model's thought process.\n","\n","4. **Applications**: Useful in arithmetic, commonsense reasoning, and symbolic tasks.\n","\n","### âœ¨ **Impact of CoT Prompting**\n","\n","- Improves accuracy and insightfulness in responses.\n","\n","- Focuses on reasoning and interpretability, especially in complex scenarios."]},{"cell_type":"markdown","metadata":{"id":"8ho8Ee15qQGw"},"source":["The code below downloads an dataset which has over [1.8 million Chain of Thought examples](https://huggingface.co/datasets/kaist-ai/CoT-Collection)."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"fCE2VgOPO_L4"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaist-ai/CoT-Collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"]}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"kaist-ai/CoT-Collection\", split=\"train\", trust_remote_code=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":672,"status":"ok","timestamp":1706678952642,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"Z0DY6By96mqc","outputId":"91050859-80ad-4f88-ce64-01d72dc78ea7"},"outputs":[],"source":[" dataset = dataset.remove_columns(['task', 'type'])\n","\n"," dataset = dataset.shuffle(seed=42)\n","\n"," dataset = dataset.select(range(10_000))\n","\n"," dataset = dataset.to_pandas()\n","\n"," dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"ATBYAzaXqfy1"},"source":["We'll sample just 10,000 of these Chain of Thought examples and save them as a list of dictionaries."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":311,"status":"ok","timestamp":1706678985658,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"duffkBFV69MV"},"outputs":[],"source":["selected_examples = dataset.to_dict(orient='records')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706678986795,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"kIFKE7Pd9YQj","outputId":"0b37eea0-e9dc-41b9-f583-02296169cc49"},"outputs":[],"source":["selected_examples[0]"]},{"cell_type":"markdown","metadata":{"id":"ctCFgLemqll9"},"source":["For this example we'll use an open source embedding model from HuggingFace so we don't accumulate a huge bill for OpenAI"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3834,"status":"ok","timestamp":1706679095210,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"qmyv-uT-EmyS"},"outputs":[],"source":["from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","\n","model_name = \"BAAI/bge-base-en-v1.5\"\n","\n","encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n","\n","embeddings = HuggingFaceBgeEmbeddings(\n","    model_name=model_name,\n","    model_kwargs={'device': 'cuda'},\n","    encode_kwargs=encode_kwargs\n",")"]},{"cell_type":"markdown","metadata":{"id":"aU69WA8pq0wb"},"source":["To set up a Chain of Thought prompt you will use a `FewShotPromptTemplate` as well as an example selector.\n","\n","In this example you'll use `MaxMarginalRelevanceExampleSelector`, but you can use any of the example selectors you learned about before."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":351,"status":"ok","timestamp":1706679404593,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"6YLFBPqMFId7"},"outputs":[],"source":["from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n","\n","from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n","\n","from langchain_community.vectorstores import FAISS"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":347,"status":"ok","timestamp":1706679442334,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"zYhYHn7NF4n_"},"outputs":[],"source":["prefix = \"Consider the following as examples of how to reason:\"\n","\n","examples_template = \"\"\"Query: {source}\n","\n","Rationale: {rationale}\n","\n","Response: {target}\n","\"\"\"\n","\n","suffix = \"\"\"Using a similar reasoning approach, answer the users question which is delimited by triple backticks.\n","\n","User question: ```{input}```\n","\n","Take a deep breath, break down the user's query step-by-step, and provide a clear chain of thought in your response.\"\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":364,"status":"ok","timestamp":1706679453143,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"nXD2hFzNH9hL"},"outputs":[],"source":["examples_prompt = PromptTemplate(\n","    input_variables=[\"source\", \"rationale\", \"target\"],\n","    template=examples_template\n",")"]},{"cell_type":"markdown","metadata":{"id":"vXuPx89GtMLv"},"source":["There's a large number of examples to embed and add to the vector store. The below cell will take a few minutes to run. It took me ~3 minutes using the free T4 GPU provided on Google colab."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":186888,"status":"ok","timestamp":1706679667969,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"5SA1Tst3IO6J"},"outputs":[],"source":["example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n","    # The list of examples available to select from.\n","    selected_examples,\n","    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n","    embeddings,\n","    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n","    FAISS,\n","    # The number of examples to produce.\n","    k=7,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1706679667970,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"cuFE8t32IX7S"},"outputs":[],"source":["mmr_prompt = FewShotPromptTemplate(\n","    example_selector=example_selector,\n","    example_prompt=examples_prompt,\n","    prefix=prefix,\n","    suffix=suffix,\n","    input_variables=[\"input\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706679667970,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"shEayc6UI0o1"},"outputs":[],"source":["query = \"\"\"Here's a short story: A lifter uses a barbell, but moves a jump rope\\\n","in a wider arc. The object likely to travel further is (A) the jump rope (B) the\\\n","barbell.\n","\n","What is the most sensical answer between \"a jump rope\" and \"a barbell\"?\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706679667970,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"KfA-c4gvJM9Q","outputId":"300a3d1d-cae1-409c-acba-0f31100b73d9"},"outputs":[],"source":["print(mmr_prompt.format(input=query))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17532,"status":"ok","timestamp":1706679778282,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"KFw8vxigJTjW","outputId":"ce515fde-f96a-45b8-eb8f-0728907b3161"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0.0)\n","\n","llm_chain = mmr_prompt | llm | StrOutputParser()\n","\n","for chunk in llm_chain.stream({\"input\":query}):\n","  print(chunk, end=\"\", flush=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1706679798740,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"FftbmTl6LgQ-"},"outputs":[],"source":["query = \"\"\"\n","Given the fact that: Inhaling, or breathing in, increases the size of the chest, \\\n"," which decreases air pressure inside the lungs. Answer the question: If Mona is \\\n"," done with a race and her chest contracts, what happens to the amount of air \\\n"," pressure in her lungs increases or decreases?\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21125,"status":"ok","timestamp":1706679821198,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"OZrMB0i0Lzif","outputId":"8f82828f-309a-4f05-acde-7a7500b1240e"},"outputs":[],"source":["for chunk in llm_chain.stream({\"input\":query}):\n","  print(chunk, end=\"\", flush=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJFjvwxWCwvX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNUpicQNfihPPdnwto66QtM","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
