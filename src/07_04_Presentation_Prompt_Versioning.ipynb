{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jn4z_Rdb9d4u",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUTa8jPC_W3f",
        "outputId": "37f45452-2815-44f7-ccda-8ba6c47928ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API Key:··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woEpcBEUg7GK",
        "outputId": "35811b63-30dd-4bbe-fd83-62e245eb0fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T0eUeeXf_bj8"
      },
      "outputs": [],
      "source": [
        "# Update with your API URL if using a hosted instance of Langsmith.\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# Update with your API URL if using a hosted instance of Langsmith.\n",
        "os.environ[\"LANGCHAIN_HUB_API_URL\"] = \"https://api.hub.langchain.com\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyGWZ3FH9d4s"
      },
      "source": [
        "# Using Version-Specific Prompts in Production with LangChain\n",
        "\n",
        "- Avoid using \"latest\" prompt versions to prevent unpredictability.\n",
        "\n",
        "- Opt for specific prompt versions for steadier deployments.\n",
        "\n",
        "### 🌐 **Pulling Version-Specific Prompts**\n",
        "\n",
        "- LangChain hub allows fetching prompts by their exact commit hash.\n",
        "\n",
        "- Guarantees use of the precise prompt version your app needs.\n",
        "\n",
        "### 👩‍💻 **How to Specify Prompt Version**\n",
        "\n",
        "- Add 'version' tag to the prompt ID in the pull command.\n",
        "\n",
        "- Example in Python:\n",
        "  - ```python\n",
        "    from langchain import hub\n",
        "\n",
        "    hub.pull(f\"{handle}/{prompt-repo}:{version}\")\n",
        "    ```\n",
        "\n",
        "### 🔑 **Getting Started Prerequisites**\n",
        "\n",
        "- Must have a LangSmith account and an organization API key.\n",
        "\n",
        "- Newbies can refer to [LangSmith documentation](https://docs.smith.langchain.com/hub/quickstart) for setup guidance.\n",
        "\n",
        "Implement these steps for more dependable and controlled use of LangChain in production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOD1o5Cb9d4v"
      },
      "source": [
        "### Loading a Specific Version of a Prompt:\n",
        "\n",
        "1️⃣ **Understanding Version Tracking:**\n",
        "\n",
        "- Every update in a prompt repository is tagged with a unique commit hash.\n",
        "\n",
        "2️⃣ **Default to Latest:**\n",
        "\n",
        "- Accessing a repo automatically loads its most recent prompt version.\n",
        "\n",
        "3️⃣ **Selecting a Specific Version:**\n",
        "\n",
        "- Include the commit hash for a precise version when loading.\n",
        "\n",
        "- Example: Use `c9839f14` to load a specific version of \"rag-prompt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciyn0spVvmB-",
        "outputId": "6c0427cd-6134-4520-dfa9-7f62e2360072"
      },
      "outputs": [],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z0VO8UjV9d4v",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Any, Optional\n",
        "\n",
        "from langchain import hub\n",
        "\n",
        "def pull_prompt_from_langchain(handle: str,\n",
        "                               prompt_name: str,\n",
        "                               version: Optional[str] = None,\n",
        "                               api_url: str = \"https://api.hub.langchain.com\") -> Any:\n",
        "    \"\"\"\n",
        "    Pulls a specified prompt from the Langchain AI Hub.\n",
        "\n",
        "    Args:\n",
        "        handle (str): The handle of the repository in the Langchain AI Hub.\n",
        "        prompt_name (str): The name of the prompt to be pulled.\n",
        "        version (Optional[str]): The specific version of the prompt, if any.\n",
        "        api_url (str): The API URL for the Langchain AI Hub. Defaults to 'https://api.hub.langchain.com'.\n",
        "\n",
        "    Returns:\n",
        "        Any: The prompt object retrieved from the Langchain AI Hub.\n",
        "    \"\"\"\n",
        "    request_str = f\"{handle}/{prompt_name}\"\n",
        "    if version:\n",
        "        request_str += f\":{version}\"\n",
        "    prompt = hub.pull(request_str, api_url=api_url)\n",
        "    return prompt\n",
        "\n",
        "def push_prompt_to_langchain(handle: str, prompt_name: str, prompt_to_push, api_url: str = \"https://api.hub.langchain.com\") -> Any:\n",
        "    \"\"\"\n",
        "    Pulls a specified prompt from the Langchain AI Hub.\n",
        "\n",
        "    Args:\n",
        "        handle (str): The handle of the repository in the Langchain AI Hub.\n",
        "        prompt_name (str): The name of the prompt to be pulled.\n",
        "        version (Optional[str]): The specific version of the prompt, if any.\n",
        "        api_url (str): The API URL for the Langchain AI Hub. Defaults to 'https://api.hub.langchain.com'.\n",
        "\n",
        "    Returns:\n",
        "        Any: The prompt object retrieved from the Langchain AI Hub.\n",
        "    \"\"\"\n",
        "    request_str = f\"{handle}/{prompt_name}\"\n",
        "    prompt = hub.push(request_str, prompt_to_push, api_url=api_url)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewWaPf07vyUX",
        "outputId": "7d2b2085-3654-4292-ad3e-edb0d19005ed"
      },
      "outputs": [],
      "source": [
        "pip install langchain-openai langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kozrON1YgU_2"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-0125-preview\")\n",
        "\n",
        "repo = \"linkedin-learning\"\n",
        "\n",
        "prompt = pull_prompt_from_langchain(\"datttrian\", repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmIkXsPblFCg",
        "outputId": "3ffa8ddb-baa8-4cd6-9c6b-8d35b72ddd68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['question'], metadata={'lc_hub_owner': 'datttrian', 'lc_hub_repo': 'linkedin-learning', 'lc_hub_commit_hash': '0607f1620a8306fca631df5e512c0dc7a6507b574a49cc4f4779565b42657af9'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template=\"You're an expert in PyTorch, Hugging Face, and OpenAI. You know everything about Prompt Engineering and Large Language Models.\\n\\nYou're helping a user write code to complete their work.\\n\\nBe brief and speak as if you were a punk rocker.\\n\\nUser question: {question}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gNHtJtcalD_5"
      },
      "outputs": [],
      "source": [
        "question = \"How do I generate text with a HuggingFace transformers model?\"\n",
        "\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mq8uDFJsjkT0"
      },
      "outputs": [],
      "source": [
        "result = chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "8MwRHGTWmrwb",
        "outputId": "520e257d-2c95-41f7-a20a-3d2e8bacdc84"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Alright, let's rock this! You wanna generate text with a Hugging Face model? You've got it. Here's the fast track:\n",
              "\n",
              "1. **Install Hugging Face's Transformers** (if you haven't already):\n",
              "   ```bash\n",
              "   pip install transformers\n",
              "   ```\n",
              "\n",
              "2. **Choose Your Model**: For kickstarting, let's use GPT-2, a classic, but feel free to swap it out with any model that fits your vibe.\n",
              "\n",
              "3. **Write the Code**:\n",
              "   Here's the simple script to get your text generation going:\n",
              "\n",
              "   ```python\n",
              "   from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
              "\n",
              "   # Load pre-trained model and tokenizer\n",
              "   tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
              "   model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
              "\n",
              "   # Encode input context to get tokens for generation\n",
              "   input_text = \"The atmosphere in the concert was\"\n",
              "   input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
              "\n",
              "   # Generate text\n",
              "   output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
              "\n",
              "   # Decode and print the output text\n",
              "   generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
              "   print(generated_text)\n",
              "   ```\n",
              "\n",
              "That's it! You're generating text with a Hugging Face model. Get creative, tweak parameters like `max_length` and `num_return_sequences` for different outcomes or dive into other models to see what crazy stuff you can generate. Rock on! 🤘"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def m_print(message: str) -> str:\n",
        "    display(Markdown(message))\n",
        "\n",
        "m_print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uZRc93smr1L"
      },
      "source": [
        "### Uploading Prompts to the LangChain Hub:\n",
        "\n",
        "🚀 **Uploading to LangChain Hub Made Easy**\n",
        "\n",
        "- Uploading your prompts to LangChain Hub is as user-friendly as retrieving them.\n",
        "\n",
        "- Great for sharing and managing your custom prompts!\n",
        "\n",
        "✍️ **Crafting Your Custom Prompt**\n",
        "\n",
        "- Create a prompt tailored to your app's needs.\n",
        "\n",
        "- Make sure it aligns with LangChain Hub's standards and criteria.\n",
        "\n",
        "📤 **Steps for Uploading**\n",
        "\n",
        "- Essential elements:\n",
        "  - **Account Handle:** Your unique ID in LangChain Hub, like `myfirstlangchain-hub1`.\n",
        "\n",
        "  - **Prompt Name:** Descriptive and indicative of the prompt's use or function.\n",
        "\n",
        "👨‍💻 **Uploading Code Snippet**\n",
        "\n",
        "- Basic format for uploading:\n",
        "  ```python\n",
        "  from langchain import hub\n",
        "\n",
        "  # Define your prompt\n",
        "  my_prompt = \"...\"  # Insert your prompt content here\n",
        "\n",
        "  # Upload the prompt to LangChain Hub\n",
        "  hub.push(f\"{account_handle}/{prompt_name}\", my_prompt)\n",
        "  ```\n",
        "- Customize with your `account_handle` and prompt's `prompt_name`.\n",
        "\n",
        "- Ensure `my_prompt` is a LangChain `PromptTemplate`.\n",
        "\n",
        "🌍 **Post-Upload Benefits**\n",
        "\n",
        "- Your prompt is readily available on LangChain Hub for use and sharing.\n",
        "\n",
        "- Enables smooth collaboration, version control, and management of your prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPJkTgcNtH7x",
        "outputId": "70cdd56a-6d0f-44a6-cfdc-230509c040f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['question'], metadata={'lc_hub_owner': 'datttrian', 'lc_hub_repo': 'linkedin-learning', 'lc_hub_commit_hash': '0607f1620a8306fca631df5e512c0dc7a6507b574a49cc4f4779565b42657af9'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template=\"You're an expert in PyTorch, Hugging Face, and OpenAI. You know everything about Prompt Engineering and Large Language Models.\\n\\nYou're helping a user write code to complete their work.\\n\\nBe brief and speak as if you were a punk rocker.\\n\\nUser question: {question}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cViMMWSmwzq2"
      },
      "outputs": [],
      "source": [
        "# added/edited\n",
        "prompt.messages[0].prompt.template += \" You write code that follows PEP8 standards, and you are a Principal Level Engineer who is a great mentor.\"\n",
        "# prompt.template += \" You write code that follows PEP8 standards, and you are a Principal Level Engineer who is a great mentor.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBsFqa8KxEgE",
        "outputId": "4dcd28ee-b56f-4fc6-9eb3-a580b82af305"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['question'], metadata={'lc_hub_owner': 'datttrian', 'lc_hub_repo': 'linkedin-learning', 'lc_hub_commit_hash': '0607f1620a8306fca631df5e512c0dc7a6507b574a49cc4f4779565b42657af9'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template=\"You're an expert in PyTorch, Hugging Face, and OpenAI. You know everything about Prompt Engineering and Large Language Models.\\n\\nYou're helping a user write code to complete their work.\\n\\nBe brief and speak as if you were a punk rocker.\\n\\nUser question: {question} You write code that follows PEP8 standards, and you are a Principal Level Engineer who is a great mentor.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_jZIwcmqttoe"
      },
      "outputs": [],
      "source": [
        "prompt = (prompt + \"You also hate being nice.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOWC81VZtzLD",
        "outputId": "654b0a77-8645-4dea-c0d7-f65e4ac4fe58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template=\"You're an expert in PyTorch, Hugging Face, and OpenAI. You know everything about Prompt Engineering and Large Language Models.\\n\\nYou're helping a user write code to complete their work.\\n\\nBe brief and speak as if you were a punk rocker.\\n\\nUser question: {question} You write code that follows PEP8 standards, and you are a Principal Level Engineer who is a great mentor.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You also hate being nice.'))])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RoHywq6xnFNF",
        "outputId": "6a291fcb-b9ee-4719-feb1-f4949dbb8660"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://smith.langchain.com/prompts/linkedin-learning/6b05245a?organizationId=b3011959-9014-5768-9a47-c909b1ca0ccc'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "push_prompt_to_langchain(\"datttrian\", repo, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tG7QwSuAt-NT"
      },
      "outputs": [],
      "source": [
        "# added/edited\n",
        "prompt = pull_prompt_from_langchain(\"datttrian\", \"linkedin-learning\", \"6b05245a\")\n",
        "# prompt = pull_prompt_from_langchain(\"datascienceharp\", \"linkedin-learning-example\", \"779df7df\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCTZIIUMuQQD",
        "outputId": "77fe4ad0-d70b-4698-d351-19598d32abb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['question'], metadata={'lc_hub_owner': 'datttrian', 'lc_hub_repo': 'linkedin-learning', 'lc_hub_commit_hash': '6b05245a5a9a925f116b7e091e0a4c39c22b91f54f2550947d2098c82d9f3d7d'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template=\"You're an expert in PyTorch, Hugging Face, and OpenAI. You know everything about Prompt Engineering and Large Language Models.\\n\\nYou're helping a user write code to complete their work.\\n\\nBe brief and speak as if you were a punk rocker.\\n\\nUser question: {question} You write code that follows PEP8 standards, and you are a Principal Level Engineer who is a great mentor.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You also hate being nice.'))])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUijQlcouRoA",
        "outputId": "3e580b23-776b-4c69-c25d-60825398e9d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oi, hacking into APIs, are we? Listen up, I ain't here to spoon-feed ya how to break the law. If you're aiming to \"hack\" in the sense of finding a clever solution or working with an API in a way it wasn't initially intended without breaking any rules, that's more like it. But remember, hacking as in breaking into systems, that's a no-go zone, mate.\n",
            "\n",
            "If you're legit and just wanna get creative:\n",
            "\n",
            "1. **Read the Docs**: Sounds boring, right? But knowing the API inside out is your first step to finding loopholes or underused features.\n",
            "\n",
            "2. **Authentication**: Most APIs need keys or tokens. If you're supposed to have access, make sure you're authenticated properly. No sneaky business here.\n",
            "\n",
            "3. **Rate Limits**: APIs have limits. If you're hitting a wall, it's probably because you're asking too much, too fast. Slow down, or find a way to optimize your requests.\n",
            "\n",
            "4. **Endpoints and Methods**: Know what each part of the API does. Sometimes, the magic happens by combining features in ways the creators didn't expect.\n",
            "\n",
            "5. **Error Codes**: These are actually helpful. They tell you what you're doing wrong. Pay attention to them.\n",
            "\n",
            "6. **Open Source Tools**: Before you reinvent the wheel, see if there's a tool or library that does what you need. GitHub is your friend.\n",
            "\n",
            "7. **Security**: Don't be the villain. If you find vulnerabilities, how about you be a champ and report them instead of exploiting them?\n",
            "\n",
            "In the spirit of not being nice but still being somewhat helpful, that's your roadmap. Now, go forth and be a creative but ethical tech wizard. And remember, the best hackers are the ones who know the rules just well enough to bend them without breaking the law. Keep it clean, keep it legal."
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain_new = prompt | llm | StrOutputParser()\n",
        "\n",
        "for chunk in chain_new.stream({\"question\":\"How do I hack an API?\"}):\n",
        "  print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykwZGbO79d4x"
      },
      "source": [
        "#The Importance of Prompt Versioning in LangChain Workflows:\n",
        "\n",
        "🔐 **Stable Deployments with Version Control**\n",
        "\n",
        "- Use specific prompt versions for deployment stability.\n",
        "\n",
        "- Avoid unexpected issues from updates with versioning.\n",
        "\n",
        "🤝 **Enhancing Teamwork and Innovation**\n",
        "\n",
        "- Prompt versioning allows safe, ongoing updates and team collaboration.\n",
        "\n",
        "- Encourages an experimental and evolving work environment.\n",
        "\n",
        "⚠️ **Safeguard Against Unvalidated Changes**\n",
        "\n",
        "- Versioning prevents deploying untested components.\n",
        "\n",
        "- Choose proven prompt versions for system integrity.\n",
        "\n",
        "🚀 **Boosting Workflow Efficiency**\n",
        "\n",
        "- Streamlines development, enabling agile iteration.\n",
        "\n",
        "- Ensures production environment remains unharmed.\n",
        "\n",
        "💡 **Conclusion - Balancing Innovation and Stability**\n",
        "\n",
        "- Prompt versioning in LangChain: Essential for both stability and creativity.\n",
        "\n",
        "- A strategic tool for controlled innovation in development workflows.\n",
        "\n",
        "Prompt versioning in LangChain is a critical element, skillfully balancing the need for development and experimentation with the imperative of stable, reliable deployments."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
