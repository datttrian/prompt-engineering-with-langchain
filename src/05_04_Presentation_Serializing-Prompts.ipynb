{"cells":[{"cell_type":"markdown","metadata":{"id":"2dxKnBFQDOXH"},"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":29865,"status":"ok","timestamp":1706646166342,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"t2wToK-qqkNB"},"outputs":[],"source":["%%capture\n","!pip install langchain==0.1.4 openai==1.10.0 langchain-openai"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3626,"status":"ok","timestamp":1706646169962,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"azqvOheLql3O","outputId":"678c60b2-bfb0-41e6-cdf6-9eda2c40a641"},"outputs":[],"source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"]},{"cell_type":"markdown","metadata":{"id":"yZ1h776jqnQ4"},"source":["# Serialization of Prompts in LangChain\n","\n","ðŸŒ **Prompt Serialization in LangChain:**\n","\n","- ðŸ“ **Save & Share Easily**: Turn prompts into files for simple storage and sharing.\n","\n","- ðŸ“„ **Choose Your Format**: Use JSON or YAML for easy-to-read files.\n","\n","- ðŸ› ï¸ **Flexible Storage**: Keep all your data in one place or spread it outâ€”it's up to you.\n","\n","- âš¡ **One-Stop Loading**: Regardless of the prompt type, loading them is a breeze.\n","\n","**Core Principles:**\n","\n","1. ðŸ‘€**Readable by Humans**: JSON and YAML make prompts easy for us to read and edit.\n","\n","2. ðŸ“š **Flexible Filing**: Whether you're a one-file wonder or a multiple-file maestro, LangChain's got you covered.\n","\n","With LangChain, managing and exchanging prompts is as smooth as sending an email!\n"]},{"cell_type":"markdown","metadata":{"id":"wIOwDys6uEiQ"},"source":["# Saving prompts"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":660,"status":"ok","timestamp":1706646510351,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"h77s5pAFuEsF"},"outputs":[],"source":["from langchain import PromptTemplate\n","\n","template = \"\"\"You are an insightful question answering bot. A user will submit\\\n","questions, which are delimited by triple backticks, and you should respond in\\\n","an insighful manner.\n","\n","Question: ```{question}```\n","\n","Take a deep breath, and think step by step before answering.\n","\n","Answer:\n","\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n","prompt.save(\"./cot_prompt.json\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1706646522109,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"IEbumrAvuE0I","outputId":"5cfa2ddf-65b2-4d6b-f738-f1e1c072f0b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["cat: /content/cot_prompt.json: No such file or directory\n"]}],"source":["!cat /content/cot_prompt.json"]},{"cell_type":"markdown","metadata":{"id":"sbcmkaNPuE63"},"source":["# Loading prompts"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":226,"status":"ok","timestamp":1706646538025,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"KwN_kiSNrnLq"},"outputs":[],"source":["# All prompts are loaded through the `load_prompt` function.\n","from langchain.prompts import load_prompt"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1706646539290,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"y596ApG8sBiY","outputId":"7df2d134-97b6-46de-b8aa-5857ad7219b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-05-02 22:12:19--  https://raw.githubusercontent.com/hwchase17/langchain-hub/master/prompts/memory/summarize/prompt.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 904 [text/plain]\n","Saving to: â€˜prompt.jsonâ€™\n","\n","prompt.json         100%[===================>]     904  --.-KB/s    in 0s      \n","\n","2024-05-02 22:12:19 (92.4 MB/s) - â€˜prompt.jsonâ€™ saved [904/904]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/hwchase17/langchain-hub/master/prompts/memory/summarize/prompt.json"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208,"status":"ok","timestamp":1706646542013,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"qEpleayXsGc5","outputId":"8d858bcb-1465-45d0-ef19-c56c8cd8de03"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","    \"input_variables\": [\n","        \"summary\",\n","        \"new_lines\"\n","    ],\n","    \"output_parser\": null,\n","    \"template\": \"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:\",\n","    \"template_format\": \"f-string\"\n","}"]}],"source":["!cat prompt.json"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133,"status":"ok","timestamp":1706646551815,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"LF8cSBSesIZh","outputId":"426dfe09-b102-4458-d3a7-c221eb22ee35"},"outputs":[{"name":"stderr","output_type":"stream","text":["No `_type` key found, defaulting to `prompt`.\n"]}],"source":["prompt = load_prompt(\"prompt.json\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1706646554941,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"Qz4fTR76sl8K","outputId":"60235c2a-04fb-4cfa-9746-502ac40e79af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n","\n","EXAMPLE\n","Current summary:\n","The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n","\n","New lines of conversation:\n","Human: Why do you think artificial intelligence is a force for good?\n","AI: Because artificial intelligence will help humans reach their full potential.\n","\n","New summary:\n","The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n","END OF EXAMPLE\n","\n","Current summary:\n","{summary}\n","\n","New lines of conversation:\n","{new_lines}\n","\n","New summary:\n"]}],"source":["print(prompt.template)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOz+9EA7GoBbsR13eqQZRIm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
