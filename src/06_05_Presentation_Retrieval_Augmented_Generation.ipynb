{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnISdj8lMqKr",
        "outputId": "4b3c2804-5fad-46f3-ae41-81e2567615f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Open AI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiCAlO7L-VlD"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "\n",
        "[Meta AI introduced the RAG method](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/), emphasizing its potential for knowledge-intensive tasks.\n",
        "\n",
        "## üîç **1. What is RAG?**\n",
        "\n",
        "- RAG, or Retrieval Augmented Generation, boosts large language models (LLMs) by tapping into external knowledge sources.\n",
        "\n",
        "- Meta AI pioneered RAG to tackle knowledge-heavy tasks efficiently.\n",
        "\n",
        "- Combines info retrieval with text generation, enabling LLMs to access fresh, reliable info.\n",
        "\n",
        "- Ideal for tasks needing accurate, current data.\n",
        "\n",
        "## ü§î **2. Why RAG was developed?**\n",
        "\n",
        "- LLMs excel in mimicking human text but face limitations.\n",
        "\n",
        "- High training/fine-tuning costs.\n",
        "\n",
        "- Knowledge is static, outdated post-training.\n",
        "\n",
        "- \"Hallucinating\" issue: confidently giving wrong info.\n",
        "\n",
        "- RAG overcomes these by merging LLM prowess with real-time data access.\n",
        "\n",
        "## 3. üõ†Ô∏è **3. How RAG Works?**\n",
        "\n",
        "- On receiving a query (like a question), RAG fetches relvant documents/passages from external sources (like Wikipedia).\n",
        "\n",
        "- Blends these retrieved docs with the query to create an enriched context. This is then processed by a text generator (e.g., GPT-3) to generate the final answer.\n",
        "\n",
        "<img src=\"https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg\">\n",
        "\n",
        "## üåü **4. Key Features of RAG:**\n",
        "\n",
        "- RAG stays current, accessing the latest info, unlike static-knowledge LLMs.\n",
        "\n",
        "- Integrates fresh info without the high cost of retraining the whole LLM.\n",
        "\n",
        "- Sources reliable info, reducing wrong answers or \"hallucinations.\"\n",
        "\n",
        "## üõ† **5. Practical Implementations:**\n",
        "\n",
        "- Answering evolving topic questions.\n",
        "\n",
        "- Useful in domains needing real-time accuracy (e.g., medical, legal).\n",
        "\n",
        "- Boosts chatbots/virtual assistants with factual, updated replies.\n",
        "\n",
        "## üìå **In Summary:**\n",
        "\n",
        "- RAG: Marrying vast LLM knowledge with the latest real-world info.\n",
        "\n",
        "- Ensures models knowledgeable, up-to-date, and accurate.\n",
        "\n",
        "# üõ†Ô∏è **RAG Implementation in LangChain**\n",
        "\n",
        "1. üß† **LLM**: The brain of the system, generating human-like text.\n",
        "\n",
        "2. üåê **Vector Store**: The heart of retrieval - stores text embeddings for quick, efficient access.\n",
        "\n",
        "3. üîç **Vector Store Retriever**: The system's \"search engine,\" finding relevant documents via vector similarities.\n",
        "\n",
        "4. üîÑ **Embedder**: Transforms text into vectors, making it readable for the system.\n",
        "\n",
        "5. üí¨ **Prompt**: Captures the initial user query or statement, kicking off the process.\n",
        "\n",
        "6. üìö **Document Loader**: Manages the import and preparation of documents for processing.\n",
        "\n",
        "7. üß© **Document Chunker**: Breaks down large documents into smaller segments for better efficiency.\n",
        "\n",
        "8. üë§ **User Input**: The starting point, where the user's query activates the RAG workflow.\n",
        "\n",
        "\n",
        "# üåê **The RAG System and Its Subsystems**\n",
        "\n",
        "### 1. üóÇÔ∏è **Index Subsystem**\n",
        "<!-- <img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\"> -->\n",
        "\n",
        "   - **Components**: Embedder, Vector Store, Document Loader, Document Chunker.\n",
        "\n",
        "   - **Function**: Processes and organizes data into an accessible format.\n",
        "\n",
        "   - **Role**: Creates a searchable database of vectorized information.\n",
        "\n",
        "### 2. üîé **Retrieval Subsystem**:\n",
        "\n",
        "   - **Components**: User Input, Prompt, Vector Store Retriever.\n",
        "\n",
        "   - **Function**: Matches user queries with relevant data.\n",
        "\n",
        "   - **Role**: Fetches the most pertinent information from the index based on user input.\n",
        "\n",
        "### 3. ü§ñ **Augment Subsystem**:\n",
        "\n",
        "   - **Components**: LLM, User Input, Retrieved Data.\n",
        "\n",
        "   - **Function**: Integrates user queries with retrieved data.\n",
        "\n",
        "   - **Role**: Generates accurate and context-rich responses, blending human-like text generation with factually correct information.\n",
        "\n",
        "<!-- <img src=\"https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png\"> -->\n",
        "\n",
        "Together, these subsystems form a seamless flow, transforming user queries into comprehensive and reliable responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK3gf7TDH9GS"
      },
      "source": [
        "# Load documents\n",
        "There are SO MANY document loaders in LangChain\n",
        "\n",
        "I won't go every single one in this notebook. But, you can check out [the documentation](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/document_loaders) to see jusy how many are available to you.\n",
        "\n",
        "## üìÇ **Understanding Document Loaders in LangChain**\n",
        "\n",
        "- üìö LangChain document loaders load data from various sources into Document objects.\n",
        "\n",
        "- üìÑ A Document is text with metadata.\n",
        "\n",
        "- üåê Loaders fetch data from text files, web pages, video transcripts, etc.\n",
        "\n",
        "- üîÑ Main role: Retrieve data for further processing.\n",
        "\n",
        "- üõ†Ô∏è Method: Use `load` to fetch data and return it as a Document.\n",
        "\n",
        "- üß† Some loaders support lazy loading (data loads into memory only when needed).\n",
        "\n",
        "## üîß **How to Use Document Loaders**\n",
        "\n",
        "1. üì• Import the loader class from `langchain.document_loaders`.\n",
        "\n",
        "2. üèóÔ∏è Create an instance of your chosen class with the directory path.\n",
        "\n",
        "3. üöÄ Use `load()` to load files in the directory into Document format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK5Pxwd7fNMm",
        "outputId": "47d3cb66-9f6b-47fb-9c47-a310d373eec3"
      },
      "outputs": [],
      "source": [
        "pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmKgqy5X_vhA",
        "outputId": "e4330d0a-aa03-40c1-e0ea-97f2d48a3c82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "yolo_nas_loader = WebBaseLoader(\"https://deci.ai/blog/pose-estimation-yolo-nas-pose/\").load()\n",
        "\n",
        "decicoder_loader = WebBaseLoader(\"https://deci.ai/blog/decicoder-6b-the-best-multi-language-code-generation-llm-in-its-class/\").load()\n",
        "\n",
        "yolo_newsletter_loader = WebBaseLoader(\"https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhwMj0CTSLli",
        "outputId": "9eafddeb-9e42-4e4d-efce-2df5cc0bf53b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUnleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Vision\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribeSign inShare this postUnleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Visiondeeplearningdaily.substack.comCopy linkFacebookEmailNoteOtherUnleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer VisionThe Future of Computer Vision is HereDeep Learning Daily CommunityMay 05, 20236Share this postUnleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Visiondeeplearningdaily.substack.comCopy linkFacebookEmailNoteOtherShareWhat does it take to make a mark in the fiercely competitive world of object detection?¬¨‚Ä†In this newsletter edition, I want to take you on a behind-the-scenes journey of how YOLO-NAS, a novel, groundbreaking object detection architecture that sets a new standard for State-of-the-Art, came into being.Igniting AmbitionOur researchers started experimenting with applying the AutoNAC engine to the object detection task in March 2023.¬¨‚Ä†Object detection, a highly competitive computer vision subfield. Many teams worldwide compete in this area, striving for state-of-the-art (SOTA) results.The team had a singular goal: to develop an object detection model to distinguish itself in a highly competitive space. The team understood that even incremental improvements could be groundbreaking, like sprinters working on shaving milliseconds off their times. In their relentless pursuit of pushing the boundaries of performance, they embraced the challenge, knowing that each incremental increase in mAP and reduction in latency could revolutionize the landscape of object detection models.As a team, we also had a secondary aim: demonstrating the capabilities of Deci's AutoNAC engine and bringing exposure to SuperGradients, our powerful, intuitive, and unparalleled training library.¬¨‚Ä†And if these graphs indicate anything, we‚Äö√Ñ√¥ve done a good job of what we set out to do.YOLO-NAS compared to previous SOTA on COCOSuperGradients star historyDesigning the BlueprintInspired by the success of modern YOLO architectures, our team set out to create a new quantization-friendly architecture - and it all started with Neural Architecture Search (NAS).NAS:Everything You Need to KnowThe first thing you need to do when performing Neural Architecture Search is define the architecture search space. For YOLO-NAS, our researchers took inspiration from the basic blocks of YOLOv6 and YOLOv8. With the architecture and training regime in place, our researchers harnessed the power of AutoNAC. It intelligently searched a vast space of ~10^14 possible architectures, ultimately zeroing in on three final networks that promised outstanding results. The result is a family of architectures with a novel quantization-friendly basic block: the small, medium, and large versions of YOLO-NAS.With the foundation of YOLO-NAS firmly established, the team was ready to embark on the next critical phase: perfecting the training regime to ensure the full potential of these innovative architectures could be realized.Interested in the technical details of YOLO-NAS? Read the technical blog post.Read the Technical Blog PostPerfecting the Training RegimeThe team devised an advanced and innovative training scheme incorporating several elements, each carefully crafted to extract maximum performance from the model:Pretraining on the extensive Object365 dataset.Employing pseudo-labelled data to enhance learning.Applying knowledge distillation from a pre-trained teacher model to improve performance.After completing the training process, post-training quantization (PTQ) was applied, converting the network into INT8.¬¨‚Ä†Testing the Model's VersatilityTo demonstrate the prowess of YOLO-NAS, we tested it on the RoboFlow 100 collection, a diverse dataset ranging from video games to aerial images.Our model outperformed close competitors like YOLOv7 and YOLOv8, proving its potential as a versatile and powerful foundation for transfer learning.Overcoming Quantization ChallengesQuantization-aware training (QAT) can enhance a model's efficiency without sacrificing accuracy, but it's far from a walk in the park.¬¨‚Ä†Learn more about QAT hereIt requires quantization-friendly model components, intricate infrastructure, and precise adjustments to settings and hyperparameters. Thankfully, our model was designed with QAT in mind, and SuperGradients' user-friendly interface and expert guidance, we not only overcame these challenges but also improved our model's efficiency without sacrificing accuracy.Tired of all the talk? Want to see YOLO-NAS in action?  Check out this getting started notebook I‚Äö√Ñ√¥ve put together for you.See YOLO-NAS in action!The LaunchpadI‚Äö√Ñ√¥m proud of the team‚Äö√Ñ√¥s achievements and the potential of YOLO-NAS, but our journey doesn't end here. As we continue to explore the boundaries of object detection (and deep learning in general), we remain committed to pushing the limits and sharing our progress with you. Our model may not remain unbeatable forever, but our unwavering determination to stay ahead of the curve will keep us on the cutting edge of this dynamic field.We‚Äö√Ñ√¥re not resting on our laurels. We rely on your support to enhance the object detection game. To achieve this, I urge you to subject YOLO-NAS and SuperGradients to stress tests, push them to their limits, and attempt to break them. Should you encounter any issues or have questions, all you have to do is open an issue on GitHub. SuperGradients is an open-source library, and if you're interested in contributing, please review our contribution guidelines. I hope this behind-the-scenes look at our efforts has piqued your interest and inspired you to follow our journey. Stay tuned for more exciting updates, and don't hesitate to reach out with any questions or feedback. The Team Behind YOLO-NASThe success of YOLO-NAS can be attributed to the hard work, dedication, and brilliance of the following individuals:Research: Amos Gropp, Ido Shahaf, Ran El-Yaniv, Akhiad BercovichEngineering: Ofri Masad, Shay Aharon, Eugene Khvedchenia, Louis Dupont, Kate YurkovaProduct: Shani Perl On behalf of the community, I thank you all for your hard work and for making YOLO-NAS a reality \\uf8ff√º√´√®\\uf8ff√º√®Œ© \\uf8ff√º√¥√®\\uf8ff√º√®Œ©!Community Generated ContentIt's impressive how quickly the community embraces and produces content with YOLO-NAS! I want to thank everyone who has shared their exceptional work so far. I know a few examples, but if you know of any additional ones, please share them.‚Äö√Ñ¬¢ DagsHub showing our integration into their platform‚Äö√Ñ¬¢ Nate Haddad integrating YOLO-NAS into Fast-Track. Check out his project on GitHub and don‚Äö√Ñ√¥t forget to give it a star ‚Äö‚â†√™√î‚àè√®!‚Äö√Ñ¬¢ Voxel51 on how to load predictions into their platform ‚Äö√Ñ¬¢ Nicolai Nielsen‚Äö√Ñ√¥s intro to YOLO-NAS video‚Äö√Ñ¬¢ Chris Alexiuk with a Star Wars Day project using YOLO-NAS‚Äö√Ñ¬¢ Code with Aarohi‚Äö√Ñ√¥s intro to YOLO-NAS video‚Äö√Ñ¬¢ Muhammad Moin‚Äö√Ñ√¥s into to YOLO-NAS video ‚Äö√Ñ¬¢ Ritesh Kanjee from Augmented Startups blog post Rise of YOLO-NAS: Discover the 8 Features That Propel It to Object Detection Stardom!‚Äö√Ñ¬¢ My starter notebooks on KaggleI‚Äö√Ñ√¥ll see you next week, cheers!Harpreet6Share this postUnleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Visiondeeplearningdaily.substack.comCopy linkFacebookEmailNoteOtherShareCommentsTopLatestDiscussionsNo postsReady for more?Subscribe¬¨¬© 2024 Deep Learning Daily CommunityPrivacy ‚Äö√†√¥ Terms ‚Äö√†√¥ Collection notice Start WritingGet the appSubstack is the home for great cultureShareCopy linkFacebookEmailNoteOther\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts\\n    \\n\\n\\n\\n\", metadata={'source': 'https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas', 'title': 'Unleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Vision', 'description': 'The Future of Computer Vision is Here', 'language': 'en'})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yolo_newsletter_loader[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1shOpId_vqb"
      },
      "source": [
        "# Chunk documents\n",
        "\n",
        "üî¢ **Exploring Text Splitters in LangChain**\n",
        "\n",
        "- üìñ Text splitters divide long texts into\n",
        "smaller, meaningful parts.\n",
        "\n",
        "- üß© Aim: Make large texts easier to handle for analysis or processing.\n",
        "\n",
        "### How Text Splitters Work:\n",
        "\n",
        "1. ‚úÇÔ∏è Split text into small, meaningful chunks (like sentences).\n",
        "\n",
        "2. üìè Combine these chunks into a larger one until a certain size is reached.\n",
        "\n",
        "3. üìå Once the size is reached, start a new chunk with some overlap for context.\n",
        "\n",
        "### Customization Axes:\n",
        "\n",
        "1. üõ†Ô∏è How the text is split.\n",
        "\n",
        "2. üìê How chunk size is measured.\n",
        "\n",
        "## Getting Started with Text Splitters\n",
        "\n",
        "- üöÄ Default choice: `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "- üìã Works by: Splitting text based on a list of characters.\n",
        "\n",
        "- üîÑ If chunks are too large, it moves to the next character.\n",
        "\n",
        "- üìå Default split characters: `[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n",
        "\n",
        "### Additional Controls:\n",
        "\n",
        "- üìè `length_function`: Defines how chunk length is calculated (default: character count, token counter is common).\n",
        "\n",
        "- üîç `chunk_size`: Sets the maximum chunk size.\n",
        "\n",
        "- üîÄ `chunk_overlap`: Determines overlap between chunks for continuity.\n",
        "\n",
        "- üìä `add_start_index`: Option to include each chunk's start position in the original document in metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fuXm06J1IDs7"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "yolo_nas_chunks = text_splitter.transform_documents(yolo_nas_loader)\n",
        "\n",
        "decicoder_chunks = text_splitter.transform_documents(decicoder_loader)\n",
        "\n",
        "yolo_newsletter_chunks = text_splitter.transform_documents(yolo_newsletter_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGzRpURwJqHM"
      },
      "source": [
        "# Index System\n",
        "\n",
        "- üéØ **Purpose:** Efficiently organize data for easy retrieval.\n",
        "\n",
        "### Steps in the Index System:\n",
        "1. üìö **Load Documents (Document Loader):**\n",
        "   - Import and read large amounts of data.\n",
        "2. üß© **Chunk Documents (Document Chunker):**\n",
        "   - Break down documents into smaller parts for better handling.\n",
        "3. üåê **Embed Documents (Embedder):**\n",
        "   - Convert text chunks into vector formats for searchability.\n",
        "4. üíæ **Store Embeddings (Vector Store):**\n",
        "   - Keep embeddings and their textual counterparts for retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwt-7kyYfcro",
        "outputId": "e11da6a5-b606-4a76-d023-746ffb2f9e56"
      },
      "outputs": [],
      "source": [
        "pip install langchain-openai faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGtfN_hiJzVB",
        "outputId": "1fbce430-aadd-4fd0-e289-7107de2f2038"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['36bb42dd-e7cb-4d99-8ba6-f83890a13967',\n",
              " '24b05637-82f0-45f0-bc12-7e574e6fcfd9',\n",
              " 'bddea5f4-3546-48e7-85ea-dba46ad73388',\n",
              " 'aa3b4318-365d-440a-b253-6214dbe396ab',\n",
              " 'b66914c4-a9e8-4475-8ee6-bcdef9d2a714',\n",
              " '11a02043-f6c8-4c04-bb90-54a6ee3779a3',\n",
              " 'f89db972-3c2f-4593-8c1e-f1aced5bad4f',\n",
              " '3a02524e-29f4-4d0d-bc39-ecd764008d15',\n",
              " '92e9e8b6-ef77-4829-89a5-c9c0d2ed870f',\n",
              " '73ce1e9f-c4b0-44d0-9e72-8270b1288ad7',\n",
              " '751d2b60-2fe0-4051-890d-063cb042cfcc',\n",
              " '4725f0bd-052f-4ade-b273-9f60828f17bb',\n",
              " '1802154a-a3cb-4100-8e4f-9afba2153606',\n",
              " 'feb8ab33-c003-418f-b65c-01ae7574f666',\n",
              " 'c2634498-3478-4d35-b97a-51a36032b9dd',\n",
              " 'cf87c96b-5cdf-4941-af49-82575d444f6a',\n",
              " '07d543a0-d9f6-43be-b7d2-a2d82c6c8bc9',\n",
              " '29633fbc-0c4f-420c-a2cc-93e88905d78a',\n",
              " 'e167cf25-8db8-4493-a6ba-dbe4e18951b0']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "store = LocalFileStore(\"./cachce/\")\n",
        "\n",
        "# create an embedder\n",
        "core_embeddings_model = OpenAIEmbeddings()\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model,\n",
        "    store,\n",
        "    namespace = core_embeddings_model.model\n",
        ")\n",
        "\n",
        "# store embeddings in vector store\n",
        "vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)\n",
        "\n",
        "vectorstore.add_documents(decicoder_chunks)\n",
        "\n",
        "vectorstore.add_documents(yolo_newsletter_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4HNkh6pmQ61"
      },
      "source": [
        "# üîç **Retrieval System**\n",
        "\n",
        "- üéØ **Purpose:** Fetch relevant information based on user queries.\n",
        "\n",
        "### Steps in the Retrieval System:\n",
        "\n",
        "1. üí¨ **Obtain User Query (User Input):**\n",
        "   - Capture the user's question or statement.\n",
        "\n",
        "2. üîÑ **Embed User Query (Embedder):**\n",
        "   - Convert the user's query into a vector format, aligning with indexed documents.\n",
        "\n",
        "3. üîç **Vector Search (Vector Store Retriever):**\n",
        "   - Search for document embeddings in the Vector Store that closely match the user query.\n",
        "\n",
        "4. üìÑ **Return Relevant Documents:**\n",
        "   - Provide the top matching documents, ensuring pertinence to the query.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Eq3nKNJ2mRDK"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4mDjaXw_6V8B"
      },
      "outputs": [],
      "source": [
        "# instantiate a retriever\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D9FGhpMWORwa"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4-0125-preview\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UgXQzj56xED"
      },
      "source": [
        "# üîç **Augment System**\n",
        "\n",
        "- üöÄ **Purpose:** Improve LLM's input with additional context.\n",
        "\n",
        "### Steps in the Augment System:\n",
        "\n",
        "1. üåü **Create Initial Prompt (Prompt):**\n",
        "   - Begin with the user's initial question or statement.\n",
        "\n",
        "2. üß© **Augment Prompt with Retrieved Context (Context Integration):**\n",
        "   - Blend the initial prompt with context from the Vector Store for a richer input.\n",
        "\n",
        "3. ‚ö° **Send Augmented Prompt to LLM (Input Enhancement):**\n",
        "   - Pass the enhanced prompt to the LLM.\n",
        "\n",
        "4. üì¨ **Receive LLM's Response (Output Reception):**\n",
        "   - Obtain the LLM's comprehensive response after processing the augmented prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gu31lNAUHJN",
        "outputId": "232548fb-f0e8-48bc-d91b-f31c9a7dd4c6"
      },
      "outputs": [],
      "source": [
        "pip install langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uBhaA_nkpU_2"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Sqnd-lUwS8",
        "outputId": "d2cd06d5-6a78-4d28-b802-874b479d7575"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSbOl4EbUXln",
        "outputId": "c9cb3dd3-da5e-40dd-e0cc-bc50d397876b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['context', 'question']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Q-NO6i0YS6nY",
        "outputId": "a34708fc-564f-49a2-832b-a5b13550184e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Deci creates its models using its advanced Neural Architecture Search (NAS) engine, AutoNAC, which automates the process of searching through vast architecture spaces efficiently. This technology allows Deci to intelligently and efficiently find optimal architectures for their models, such as DeciCoder-6B and YOLO-NAS, by navigating through potentially trillions of possible architectures. AutoNAC significantly reduces the necessary computational resources compared to traditional NAS methods, making the development of high-performing neural networks more practical and accessible.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is the entire augment system!\n",
        "rag_chain.invoke(\"What does Neural Architecture Search have to do with how Deci creates its models?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "jGBkR1ewxHWY",
        "outputId": "e30596b3-718d-453d-b701-85afac636484"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DeciCoder is a cost-effective, highly efficient, and accurate code generation application, notable for its 6-billion parameter model, DeciCoder-6B. It is designed for performance at scale, outperforming competitors like CodeGen and StarCoder in terms of both speed and accuracy, particularly in Python. DeciCoder-6B achieves significant computational efficiency and low latency, making it a superior choice for scaling code generation tasks.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke(\"What is DeciCoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9DNus0R7mSP"
      },
      "source": [
        "## Return sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sDTWLOfk7mXX"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "rag_chain_from_docs = (\n",
        "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain_with_source = RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()}).assign(answer=rag_chain_from_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MTFGSfI7mbm",
        "outputId": "39937ee5-72c2-4f5e-93b1-8f91153e297b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': [Document(page_content='which is a product of Deci‚Äôs cutting-edge Neural Architecture Search-based AutoNAC engine.', metadata={'source': 'https://deci.ai/blog/decicoder-6b-the-best-multi-language-code-generation-llm-in-its-class/', 'title': 'Introducing DeciCoder-6B: Code LLM Engineered for Accuracy & Cost Efficiency At Scale', 'description': 'DeciCoder-6B, a multi-language code LLM in the 7B parameter class that supports a sequence length of up to 4096 tokens and excels in 8 code languages.', 'language': 'en-US'}),\n",
              "  Document(page_content='Neural Architecture Search is define the architecture search space. For YOLO-NAS, our researchers took inspiration from the basic blocks of YOLOv6 and YOLOv8. With the architecture and training regime in place, our researchers harnessed the power of AutoNAC. It intelligently searched a vast space of ~10^14 possible architectures, ultimately zeroing in on three final networks that promised outstanding results. The result is a family of architectures with a novel quantization-friendly basic', metadata={'source': 'https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas', 'title': 'Unleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Vision', 'description': 'The Future of Computer Vision is Here', 'language': 'en'}),\n",
              "  Document(page_content='The NAS Engine Behind DeciCoder-6B: AutoNAC\\nThe architecture of DeciCoder-6B was developed using Deci‚Äôs advanced Neural Architecture Search (NAS) engine, AutoNAC. Traditional NAS methods, while promising, typically require extensive computational resources. AutoNAC circumvents this challenge by automating the search process in a more compute-efficient manner.', metadata={'source': 'https://deci.ai/blog/decicoder-6b-the-best-multi-language-code-generation-llm-in-its-class/', 'title': 'Introducing DeciCoder-6B: Code LLM Engineered for Accuracy & Cost Efficiency At Scale', 'description': 'DeciCoder-6B, a multi-language code LLM in the 7B parameter class that supports a sequence length of up to 4096 tokens and excels in 8 code languages.', 'language': 'en-US'}),\n",
              "  Document(page_content='Harnessing AutoNAC for novel head design\\nFinding an optimal architecture requires navigating the vast architecture search space. But doing so manually is not only tedious but often inefficient. The traditional Neural Architecture Search showed promise in this arena, automating the development of superior neural networks. However, its insatiable thirst for computational resources rendered it impractical for many, confining its usage to a select few with vast computational power.', metadata={'source': 'https://deci.ai/blog/pose-estimation-yolo-nas-pose/', 'title': 'YOLO-NAS Pose - SOTA Pose Estimation Model Using NAS | Deci', 'description': \"Building on YOLO-NAS, we've unveiled its pose estimation sibling: YOLO-NAS Pose. This model offers a superior latency-accuracy balance compared to YOLOv8 Pose.\", 'language': 'en-US'})],\n",
              " 'question': 'What does Neural Architecture Search have to do with how Deci creates its models?',\n",
              " 'answer': 'Neural Architecture Search (NAS) is integral to how Deci creates its models through their proprietary AutoNAC engine, which automates the search for optimal neural network architectures in a compute-efficient manner. This process involves navigating a vast space of potential architectures, where AutoNAC intelligently selects the most promising designs for specific tasks. Deci leverages NAS to develop models like DeciCoder-6B and YOLO-NAS, optimizing them for performance and efficiency without the extensive computational resources typically required by traditional NAS methods.'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain_with_source.invoke(\"What does Neural Architecture Search have to do with how Deci creates its models?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
