{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_3lRnm7gCmQe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain==0.1.1 openai==1.8.0 langchain-openai tiktoken faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRC_1Ifuz1f9",
        "outputId": "8269bc24-ba72-4056-ef74-6b6518cdfc6e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "aiohttp==3.9.5\n",
            "aiosignal==1.3.1\n",
            "alabaster==0.7.16\n",
            "albumentations==1.3.1\n",
            "altair==4.2.2\n",
            "annotated-types==0.6.0\n",
            "anyio==3.7.1\n",
            "appdirs==1.4.4\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.5.1\n",
            "arviz==0.15.1\n",
            "astropy==5.3.4\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.3\n",
            "atpublic==4.1.0\n",
            "attrs==23.2.0\n",
            "audioread==3.0.1\n",
            "autograd==1.6.2\n",
            "Babel==2.15.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.12.3\n",
            "bidict==0.23.1\n",
            "bigframes==1.6.0\n",
            "bleach==6.1.0\n",
            "blinker==1.4\n",
            "blis==0.7.11\n",
            "blosc2==2.0.0\n",
            "bokeh==3.3.4\n",
            "bqplot==0.12.43\n",
            "branca==0.7.2\n",
            "build==1.2.1\n",
            "CacheControl==0.14.0\n",
            "cachetools==5.3.3\n",
            "catalogue==2.0.10\n",
            "certifi==2024.2.2\n",
            "cffi==1.16.0\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.3.2\n",
            "chex==0.1.86\n",
            "click==8.1.7\n",
            "click-plugins==1.1.1\n",
            "cligj==0.7.2\n",
            "cloudpathlib==0.16.0\n",
            "cloudpickle==2.2.1\n",
            "cmake==3.27.9\n",
            "cmdstanpy==1.2.2\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.4\n",
            "cons==0.4.6\n",
            "contextlib2==21.6.0\n",
            "contourpy==1.2.1\n",
            "cryptography==42.0.7\n",
            "cuda-python==12.2.1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-24.4.1-cp310-cp310-manylinux_2_28_x86_64.whl#sha256=57366e7ef09dc63e0b389aff20df6c37d91e2790065861ee31a4720149f5b694\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda12x==12.2.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.3.4\n",
            "cycler==0.12.1\n",
            "cymem==2.0.8\n",
            "Cython==3.0.10\n",
            "dask==2023.8.1\n",
            "dataclasses-json==0.6.6\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.2.0\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.6.6\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "distributed==2023.8.1\n",
            "distro==1.7.0\n",
            "dlib==19.24.4\n",
            "dm-tree==0.1.8\n",
            "docstring_parser==0.16\n",
            "docutils==0.18.1\n",
            "dopamine_rl==4.0.9\n",
            "duckdb==0.10.2\n",
            "earthengine-api==0.1.403\n",
            "easydict==1.13\n",
            "ecos==2.0.13\n",
            "editdistance==0.6.2\n",
            "eerepr==0.0.4\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\n",
            "entrypoints==0.4\n",
            "et-xmlfile==1.1.0\n",
            "etils==1.7.0\n",
            "etuples==0.3.9\n",
            "exceptiongroup==1.2.1\n",
            "faiss-cpu==1.8.0\n",
            "fastai==2.7.15\n",
            "fastcore==1.5.37\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.19.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.2\n",
            "filelock==3.14.0\n",
            "fiona==1.9.6\n",
            "firebase-admin==5.3.0\n",
            "Flask==2.2.5\n",
            "flatbuffers==24.3.25\n",
            "flax==0.8.3\n",
            "folium==0.14.0\n",
            "fonttools==4.51.0\n",
            "frozendict==2.4.4\n",
            "frozenlist==1.4.1\n",
            "fsspec==2023.6.0\n",
            "future==0.18.3\n",
            "gast==0.5.4\n",
            "gcsfs==2023.6.0\n",
            "GDAL==3.6.4\n",
            "gdown==5.1.0\n",
            "geemap==0.32.1\n",
            "gensim==4.3.2\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==0.13.2\n",
            "geopy==2.3.0\n",
            "gin-config==0.5.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.4\n",
            "google-api-core==2.11.1\n",
            "google-api-python-client==2.84.0\n",
            "google-auth==2.27.0\n",
            "google-auth-httplib2==0.1.1\n",
            "google-auth-oauthlib==1.2.0\n",
            "google-cloud-aiplatform==1.51.0\n",
            "google-cloud-bigquery==3.21.0\n",
            "google-cloud-bigquery-connection==1.12.1\n",
            "google-cloud-bigquery-storage==2.25.0\n",
            "google-cloud-core==2.3.3\n",
            "google-cloud-datastore==2.15.2\n",
            "google-cloud-firestore==2.11.1\n",
            "google-cloud-functions==1.13.3\n",
            "google-cloud-iam==2.15.0\n",
            "google-cloud-language==2.13.3\n",
            "google-cloud-resource-manager==1.12.3\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.11.3\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=936ceeb69d19b2724915651ed0ef73ba3e514d18160c86cd778945c1a2e9a7bf\n",
            "google-crc32c==1.5.0\n",
            "google-generativeai==0.5.4\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.0\n",
            "googleapis-common-protos==1.63.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.3\n",
            "greenlet==3.0.3\n",
            "grpc-google-iam-v1==0.13.0\n",
            "grpcio==1.63.0\n",
            "grpcio-status==1.48.2\n",
            "gspread==6.0.2\n",
            "gspread-dataframe==3.3.1\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h11==0.14.0\n",
            "h5netcdf==1.3.0\n",
            "h5py==3.9.0\n",
            "holidays==0.48\n",
            "holoviews==1.17.1\n",
            "html5lib==1.1\n",
            "httpcore==1.0.5\n",
            "httpimport==1.3.1\n",
            "httplib2==0.22.0\n",
            "httpx==0.27.0\n",
            "huggingface-hub==0.23.0\n",
            "humanize==4.7.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==8.0.0\n",
            "idna==3.7\n",
            "imageio==2.31.6\n",
            "imageio-ffmpeg==0.4.9\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.10.1\n",
            "imgaug==0.4.0\n",
            "importlib_metadata==7.1.0\n",
            "importlib_resources==6.4.0\n",
            "imutils==0.5.4\n",
            "inflect==7.0.0\n",
            "iniconfig==2.0.0\n",
            "intel-openmp==2023.2.4\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==5.5.6\n",
            "ipyleaflet==0.18.2\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jax==0.4.26\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda12/jaxlib-0.4.26+cuda12.cudnn89-cp310-cp310-manylinux2014_x86_64.whl#sha256=813cf1fe3e7ca4dbf5327d6e7b4fc8521e92d8bba073ee645ae0d5d036a25750\n",
            "jeepney==0.7.1\n",
            "jellyfish==1.0.3\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.4\n",
            "joblib==1.4.2\n",
            "jsonpatch==1.33\n",
            "jsonpickle==3.0.4\n",
            "jsonpointer==2.4\n",
            "jsonschema==4.19.2\n",
            "jsonschema-specifications==2023.12.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-server==1.24.0\n",
            "jupyter_core==5.7.2\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.10\n",
            "kaggle==1.6.14\n",
            "kagglehub==0.2.5\n",
            "keras==2.15.0\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.5\n",
            "langchain==0.1.1\n",
            "langchain-community==0.0.20\n",
            "langchain-core==0.1.23\n",
            "langchain-openai==0.0.4\n",
            "langcodes==3.4.0\n",
            "langsmith==0.0.87\n",
            "language_data==1.2.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "librosa==0.10.2.post1\n",
            "lightgbm==4.1.0\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.41.1\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==4.9.4\n",
            "malloy==2023.1067\n",
            "marisa-trie==1.1.1\n",
            "Markdown==3.6\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==2.1.5\n",
            "marshmallow==3.21.2\n",
            "matplotlib==3.7.1\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==0.11.10\n",
            "mdit-py-plugins==0.4.1\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==0.8.4\n",
            "mizani==0.9.3\n",
            "mkl==2023.2.0\n",
            "ml-dtypes==0.2.0\n",
            "mlxtend==0.22.0\n",
            "more-itertools==10.1.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.0.8\n",
            "multidict==6.0.5\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.10\n",
            "music21==9.1.0\n",
            "mypy-extensions==1.0.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.0.0\n",
            "nbclient==0.10.0\n",
            "nbconvert==6.5.4\n",
            "nbformat==5.10.4\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.3\n",
            "nibabel==4.0.2\n",
            "nltk==3.8.1\n",
            "notebook==6.5.5\n",
            "notebook_shim==0.2.4\n",
            "numba==0.58.1\n",
            "numexpr==2.10.0\n",
            "numpy==1.25.2\n",
            "nvtx==0.2.10\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "openai==1.8.0\n",
            "opencv-contrib-python==4.8.0.76\n",
            "opencv-python==4.8.0.76\n",
            "opencv-python-headless==4.9.0.80\n",
            "openpyxl==3.1.2\n",
            "opt-einsum==3.3.0\n",
            "optax==0.2.2\n",
            "orbax-checkpoint==0.4.4\n",
            "osqp==0.6.2.post8\n",
            "packaging==23.2\n",
            "pandas==2.0.3\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.19.2\n",
            "pandas-stubs==2.0.3.230814\n",
            "pandocfilters==1.5.1\n",
            "panel==1.3.8\n",
            "param==2.1.0\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.6\n",
            "peewee==3.17.5\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==9.4.0\n",
            "pip-tools==6.13.0\n",
            "platformdirs==4.2.2\n",
            "plotly==5.15.0\n",
            "plotnine==0.12.4\n",
            "pluggy==1.5.0\n",
            "polars==0.20.2\n",
            "pooch==1.8.1\n",
            "portpicker==1.5.2\n",
            "prefetch-generator==1.0.3\n",
            "preshed==3.0.9\n",
            "prettytable==3.10.0\n",
            "proglog==0.1.10\n",
            "progressbar2==4.2.0\n",
            "prometheus_client==0.20.0\n",
            "promise==2.3\n",
            "prompt-toolkit==3.0.43\n",
            "prophet==1.1.5\n",
            "proto-plus==1.23.0\n",
            "protobuf==3.20.3\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.9\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==14.0.2\n",
            "pyarrow-hotfix==0.6\n",
            "pyasn1==0.6.0\n",
            "pyasn1_modules==0.4.0\n",
            "pycocotools==2.0.7\n",
            "pycparser==2.22\n",
            "pydantic==2.7.1\n",
            "pydantic_core==2.18.2\n",
            "pydata-google-auth==1.8.2\n",
            "pydot==1.4.2\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.6.3\n",
            "pyerfa==2.0.1.4\n",
            "pygame==2.5.2\n",
            "Pygments==2.16.1\n",
            "PyGObject==3.42.1\n",
            "PyJWT==2.3.0\n",
            "pymc==5.10.4\n",
            "pymystem3==0.2.0\n",
            "pynvjitlink-cu12==0.2.3\n",
            "PyOpenGL==3.1.7\n",
            "pyOpenSSL==24.1.0\n",
            "pyparsing==3.1.2\n",
            "pyperclip==1.8.2\n",
            "pyproj==3.6.1\n",
            "pyproject_hooks==1.1.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pytensor==2.18.6\n",
            "pytest==7.4.4\n",
            "python-apt @ file:///backend-container/containers/python_apt-0.0.0-cp310-cp310-linux_x86_64.whl#sha256=b209c7165d6061963abe611492f8c91c3bcef4b7a6600f966bab58900c63fefa\n",
            "python-box==7.1.1\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.4\n",
            "python-utils==3.8.2\n",
            "pytz==2023.4\n",
            "pyviz_comms==3.0.2\n",
            "PyWavelets==1.6.0\n",
            "PyYAML==6.0.1\n",
            "pyzmq==24.0.1\n",
            "qdldl==0.1.7.post2\n",
            "qudida==0.0.4\n",
            "ratelim==0.1.6\n",
            "referencing==0.35.1\n",
            "regex==2023.12.25\n",
            "requests==2.31.0\n",
            "requests-oauthlib==1.3.1\n",
            "requirements-parser==0.9.0\n",
            "rich==13.7.1\n",
            "rmm-cu12==24.4.0\n",
            "rpds-py==0.18.1\n",
            "rpy2==3.4.2\n",
            "rsa==4.9\n",
            "safetensors==0.4.3\n",
            "scikit-image==0.19.3\n",
            "scikit-learn==1.2.2\n",
            "scipy==1.11.4\n",
            "scooby==0.10.0\n",
            "scs==3.2.4.post1\n",
            "seaborn==0.13.1\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.3\n",
            "sentencepiece==0.1.99\n",
            "shapely==2.0.4\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "smart-open==6.4.0\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.5\n",
            "soxr==0.3.7\n",
            "spacy==3.7.4\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "Sphinx==5.0.2\n",
            "sphinxcontrib-applehelp==1.0.8\n",
            "sphinxcontrib-devhelp==1.0.6\n",
            "sphinxcontrib-htmlhelp==2.0.5\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.7\n",
            "sphinxcontrib-serializinghtml==1.1.10\n",
            "SQLAlchemy==2.0.30\n",
            "sqlglot==20.11.0\n",
            "sqlparse==0.5.0\n",
            "srsly==2.4.8\n",
            "stanio==0.5.0\n",
            "statsmodels==0.14.2\n",
            "StrEnum==0.4.15\n",
            "sympy==1.12\n",
            "tables==3.8.0\n",
            "tabulate==0.9.0\n",
            "tbb==2021.12.0\n",
            "tblib==3.0.0\n",
            "tenacity==8.3.0\n",
            "tensorboard==2.15.2\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow @ https://storage.googleapis.com/colab-tf-builds-public-09h6ksrfwbb9g9xv/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl#sha256=a2ec79931350b378c1ef300ca836b52a55751acb71a433582508a07f0de57c42\n",
            "tensorflow-datasets==4.9.4\n",
            "tensorflow-estimator==2.15.0\n",
            "tensorflow-gcs-config==2.15.0\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.0\n",
            "tensorflow-metadata==1.15.0\n",
            "tensorflow-probability==0.23.0\n",
            "tensorstore==0.1.45\n",
            "termcolor==2.4.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.15.1\n",
            "thinc==8.2.3\n",
            "threadpoolctl==3.5.0\n",
            "tifffile==2024.5.10\n",
            "tiktoken==0.5.2\n",
            "tinycss2==1.3.0\n",
            "tokenizers==0.19.1\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.1\n",
            "torch @ https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=0a12aa9aa6bc442dff8823ac8b48d991fd0771562eaa38593f9c8196d65f7007\n",
            "torchaudio @ https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=38b49393f8c322dcaa29d19e5acbf5a0b1978cf1b719445ab670f1fb486e3aa6\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.18.0\n",
            "torchvision @ https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=13e1b48dc5ce41ccb8100ab3dd26fdf31d8f1e904ecf2865ac524493013d0df5\n",
            "tornado==6.3.3\n",
            "tqdm==4.66.4\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.41.0\n",
            "triton==2.3.0\n",
            "tweepy==4.14.0\n",
            "typer==0.9.4\n",
            "types-pytz==2024.1.0.20240417\n",
            "types-setuptools==69.5.0.20240519\n",
            "typing-inspect==0.9.0\n",
            "typing_extensions==4.11.0\n",
            "tzdata==2024.1\n",
            "tzlocal==5.2\n",
            "uc-micro-py==1.0.3\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.0.7\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wasabi==1.1.2\n",
            "wcwidth==0.2.13\n",
            "weasel==0.3.4\n",
            "webcolors==1.13\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "Werkzeug==3.0.3\n",
            "widgetsnbextension==3.6.6\n",
            "wordcloud==1.9.3\n",
            "wrapt==1.14.1\n",
            "xarray==2023.7.0\n",
            "xarray-einstats==0.7.0\n",
            "xgboost==2.0.3\n",
            "xlrd==2.0.1\n",
            "xyzservices==2024.4.0\n",
            "yarl==1.9.4\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.38\n",
            "zict==3.0.0\n",
            "zipp==3.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
      ],
      "metadata": {
        "id": "pP5m6RUcCqpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd0003d-e985-41ae-b316-370b2a061750"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example selectors\n",
        "\n",
        "Example selectors in Langchain are classes that are responsible for selecting which examples to include in a prompt.\n",
        "\n",
        "They are useful when you have a large number of examples available, but need to select a subset of them to include in your prompt.\n",
        "\n",
        "### Some key things about example selectors:\n",
        "\n",
        "They implement a `select_examples` method that takes in the input variables and returns a list of examples to include in the prompt.\n",
        "\n",
        "#### There are different strategies for selecting examples, such as:\n",
        "\n",
        " - Selecting by semantic similarity to the inputs (`SemanticSimilarityExampleSelector`)\n",
        "\n",
        " - Selecting by maximal marginal relevance to balance similarity and diversity (`MaxMarginalRelevanceExampleSelector`)\n",
        "\n",
        " - Selecting based on prompt length (`LengthBasedExampleSelector`)\n",
        "\n",
        "Example selectors allow prompts to dynamically choose examples based on the inputs, rather than having fixed examples.\n",
        "\n",
        "They help manage long prompts by only including the most relevant examples for the given inputs.\n",
        "\n",
        "New example selectors can be implemented by subclassing `BaseExampleSelector` and defining a custom select_examples method.\n",
        "\n",
        "Example selectors provide a way to dynamically select the most relevant examples to include in a prompt for given inputs, rather than using a fixed set of examples.\n",
        "\n",
        "This helps manage prompt length and improve relevance.\n",
        "\n",
        "## BaseExampleSelector\n",
        "\n",
        "The base interface is defined as below.\n",
        "\n",
        "The only method it needs to define is a `select_examples` method. This takes in the input variables and then returns a list of examples.\n",
        "\n",
        "It is up to each specific implementation as to how those examples are selected.\n",
        "\n",
        "```python\n",
        "class BaseExampleSelector(ABC):\n",
        "    \"\"\"Interface for selecting examples to include in prompts.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
        "        \"\"\"Select which examples to use based on the inputs.\"\"\"\n",
        "```\n",
        "\n",
        "\n",
        "# Implementing a Custom Example Selector\n",
        "\n",
        "Get excited. Because now you're about to create a custom example selector that randomly picks examples from a provided list.\n",
        "\n",
        "## Requirements for an `ExampleSelector`:\n",
        "\n",
        "An `ExampleSelector` in LangChain needs to implement two primary methods:\n",
        "\n",
        "1. `add_example`: This method accepts an example and integrates it into the `ExampleSelector`.\n",
        "\n",
        "2. `select_examples`: Given input variables (typically user input), this method returns a list of examples to be used in a few-shot prompt.\n",
        "\n",
        "## Custom Implementation:\n",
        "\n",
        "Here's how we can create a custom example selector that randomly selects examples:\n"
      ],
      "metadata": {
        "id": "n7wurWIyEboL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
        "from typing import Dict, List\n",
        "import numpy as np\n",
        "\n",
        "class CustomExampleSelector(BaseExampleSelector):\n",
        "    def __init__(self, examples: List[Dict[str, str]]):\n",
        "        self.examples = examples\n",
        "\n",
        "    def add_example(self, example: Dict[str, str]) -> None:\n",
        "        \"\"\"Add a new example to the list.\"\"\"\n",
        "        self.examples.append(example)\n",
        "\n",
        "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
        "        \"\"\"Randomly select examples based on the provided inputs.\"\"\"\n",
        "        return list(np.random.choice(self.examples, size=2, replace=False))"
      ],
      "metadata": {
        "id": "yyY7tNIHGVHq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\"recipe\": \"Spaghetti Carbonara\"},\n",
        "    {\"recipe\": \"Chicken Alfredo\"},\n",
        "    {\"recipe\": \"Vegetable Stir Fry\"}\n",
        "]"
      ],
      "metadata": {
        "id": "np4JO8sjSwCV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize an example selector\n",
        "example_selector = CustomExampleSelector(examples)"
      ],
      "metadata": {
        "id": "5QTFV1rdTM2R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The use of `{\"recipe\": \"recipe\"}` in the example a placeholder or a generic representation, and it doesn't affect the outcome of the method.\n",
        "\n",
        "In a more sophisticated example selector, the input variables might be used to filter or prioritize the selection of examples based on certain criteria.\n",
        "\n",
        "But in the provided example, it's not utilized.\n",
        "\n"
      ],
      "metadata": {
        "id": "cdiW8_RRUV9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly select some examples\n",
        "example_selector.select_examples({\"recipe\": \"recipe\"})"
      ],
      "metadata": {
        "id": "U8JZisP5TU9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8a349a-a2ee-4185-c1c8-24ee6133ad0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'recipe': 'Spaghetti Carbonara'}, {'recipe': 'Vegetable Stir Fry'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new recipe to the collection\n",
        "example_selector.add_example({\"recipe\": \"Beef Stroganoff\"})\n",
        "print(example_selector.examples)"
      ],
      "metadata": {
        "id": "-Dkp-OvUTtXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58788335-1370-495c-e6b5-d88f618dbfd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'recipe': 'Spaghetti Carbonara'}, {'recipe': 'Chicken Alfredo'}, {'recipe': 'Vegetable Stir Fry'}, {'recipe': 'Beef Stroganoff'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select two recipes again\n",
        "example_selector.select_examples({\"recipe\": \"recipe\"})"
      ],
      "metadata": {
        "id": "StDIUhLJT0xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b7497b-01c5-4012-8643-e77c23d08638"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'recipe': 'Beef Stroganoff'}, {'recipe': 'Vegetable Stir Fry'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select by length\n",
        "\n",
        "Ready for more? Because below you will implement a LengthBasedExampleSelector which selects examples based on their length.\n",
        "\n",
        "This is particularly useful when you're worried about the total length of the constructed prompt, especially given the context window limitations of some models."
      ],
      "metadata": {
        "id": "lrh9R0UBT53u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts import FewShotPromptTemplate\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"caffeinated\", \"output\": \"sleepy\"},\n",
        "    {\"input\": \"spooky\", \"output\": \"cuddly\"},\n",
        "    {\"input\": \"crispy\", \"output\": \"soggy\"},\n",
        "    {\"input\": \"galactic\", \"output\": \"mundane\"},\n",
        "    {\"input\": \"funky\", \"output\": \"plain-jane\"},\n",
        "]"
      ],
      "metadata": {
        "id": "CqLxq-6zU54F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template = \"Input: {input}\\nOutput: {output}\"\n",
        ")"
      ],
      "metadata": {
        "id": "--pL90juU8fR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LengthBasedExampleSelector\n",
        "\n",
        "The `LengthBasedExampleSelector` is an example selector that selects examples based on the length of the formatted examples.\n",
        "\n",
        "The primary goal of this selector is to adjust the number of examples included in the prompt based on the length of the input and the examples themselves.\n",
        "\n",
        "This is useful when there's a concern about the total length of the constructed prompt, especially given the context window limitations of some models.\n",
        "\n",
        "## It works as follows:\n",
        "\n",
        "When adding a new example using `add_example`, the example is formatted using the provided `example_prompt`, and its length is calculated using the `get_text_length function`.\n",
        "\n",
        "This length is then stored in the `example_text_lengths` list.\n",
        "\n",
        "When selecting examples using `select_examples`, the method first calculates the length of the provided input.\n",
        "\n",
        "It then determines how much length is left for examples by subtracting the input length from the `max_length`.\n",
        "\n",
        "The method then iteratively checks each example's length against the remaining length.\n",
        "\n",
        "If the example fits, it's added to the list of selected examples, and its length is subtracted from the remaining length.\n",
        "\n",
        "This process continues until the remaining length is exhausted or all examples have been considered.\n",
        "\n",
        "This allows it to dynamically select more examples for shorter inputs and fewer examples for longer inputs, to try to keep the overall prompt length under max_length.\n",
        "\n",
        "So in summary, it selects examples based on length to try to construct prompts that don't exceed the context window size of the model."
      ],
      "metadata": {
        "id": "wRnjPfOHX9ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt= example_prompt,\n",
        "    max_length=15,\n",
        ")"
      ],
      "metadata": {
        "id": "DPtSWwx6VjRr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"What's the opposite of...\",\n",
        "    suffix=\"Input: {adjective}\\nOutput:\",\n",
        "    input_variables=[\"adjective\"],\n",
        ")"
      ],
      "metadata": {
        "id": "bZyX2kRDVz_R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_string = \"fluffy and puffy and cloud-like and soft as a marshmallow and as cuddly as a teddy bear in a world of cotton candy\"\n",
        "print(dynamic_prompt.format(adjective=long_string))"
      ],
      "metadata": {
        "id": "XyIeIa3EWBhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f2ca12-a190-4200-fdb8-e2852ab8c772"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What's the opposite of...\n",
            "\n",
            "Input: fluffy and puffy and cloud-like and soft as a marshmallow and as cuddly as a teddy bear in a world of cotton candy\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting based on Maximal Marginal Relevance (MMR)\n",
        "\n",
        "Maximal Marginal Relevance (MMR) is a technique often used in information retrieval to balance between the relevance of documents (or examples, in this case) and the diversity among them.  \n",
        "\n",
        "The idea is to avoid redundancy in the selected set of documents while ensuring that the documents are still relevant to the query.\n",
        "\n",
        "### Working:\n",
        "\n",
        "1. **Relevance Calculation**:\n",
        "   - For each example, calculate its cosine similarity with the input. This gives a measure of how relevant each example is to the input.\n",
        "\n",
        "2. **Diversity Calculation**:\n",
        "   - For each example, calculate its cosine similarity with already selected examples. This gives a measure of how similar the example is to what has already been chosen.\n",
        "\n",
        "3. **MMR Score Calculation**:\n",
        "   - For each example, compute its MMR score as a combination of its relevance to the input and its diversity from already selected examples. The formula is typically: $$ MMR = \\lambda \\times Relevance - (1 - \\lambda) \\times Diversity $$\n",
        "   \n",
        "   - Where `λ` is a parameter between 0 and 1 that controls the trade-off between relevance and diversity.\n",
        "\n",
        "4. **Example Selection**:\n",
        "   - Iteratively select the example with the highest MMR score, add it to the selected set, and update the diversity calculations for the remaining examples.\n",
        "\n",
        "5. **Termination**:\n",
        "   - Continue the process until a stopping criterion is met, such as a predefined number of examples or until the MMR score falls below a threshold.\n"
      ],
      "metadata": {
        "id": "Vs7fPTsCWGmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import (\n",
        "    MaxMarginalRelevanceExampleSelector,\n",
        "    SemanticSimilarityExampleSelector,\n",
        ")\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
      ],
      "metadata": {
        "id": "hXakMuWia1ow"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples"
      ],
      "metadata": {
        "id": "dGhgJGD0aff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb50f96-264e-4650-9122-1f64959eae14"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': 'caffeinated', 'output': 'sleepy'},\n",
              " {'input': 'spooky', 'output': 'cuddly'},\n",
              " {'input': 'crispy', 'output': 'soggy'},\n",
              " {'input': 'galactic', 'output': 'mundane'},\n",
              " {'input': 'funky', 'output': 'plain-jane'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt"
      ],
      "metadata": {
        "id": "fUoBPBp1aktk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e151f45-cbc4-45d7-e4c9-e47abcdda781"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['input', 'output'], template='Input: {input}\\nOutput: {output}')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MaxMarginalRelevanceExampleSelector Overview\n",
        "\n",
        "`MaxMarginalRelevanceExampleSelector` is a specialized class designed to select examples based on the Max Marginal Relevance (MMR) criterion. MMR is a method that balances the trade-off between:\n",
        "\n",
        "- **Relevance**: How similar an example is to a given query.\n",
        "- **Diversity**: How different the selected examples are from each other.\n",
        "\n",
        "This approach provides a comprehensive set of examples that are both relevant to a query and diverse.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "1. **Derived from Semantic Similarity**:\n",
        "   - Inherits from `SemanticSimilarityExampleSelector`.\n",
        "   - Utilizes basic functionality of selecting examples based on their semantic similarity to a given query.\n",
        "\n",
        "2. **Attributes**:\n",
        "   - `fetch_k`: Number of examples to initially fetch before reranking them using MMR.\n",
        "\n",
        "3. **Methods**:\n",
        "\n",
        "   - **`select_examples(input_variables: Dict[str, str]) -> List[dict]`**:\n",
        "     - Constructs a query from the provided input variables.\n",
        "     - Fetches top `fetch_k` examples and reranks them using MMR to select the top `k` examples.\n",
        "     - Extracts examples from metadata and filters based on `example_keys` if provided.\n",
        "\n",
        "   - **`from_examples(...) -> MaxMarginalRelevanceExampleSelector`**:\n",
        "     - Class method to create an instance.\n",
        "     - Initializes using a list of examples, embeddings, and other parameters.\n",
        "     - Sets up the vector store with provided examples and embeddings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_bVSaUO-dtzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mmr_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
        "    examples,\n",
        "    OpenAIEmbeddings(),\n",
        "    FAISS,\n",
        "    k=2\n",
        ")"
      ],
      "metadata": {
        "id": "0GPIRqvgaoPW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmr_prompt = FewShotPromptTemplate(\n",
        "    # We provide an ExampleSelector instead of examples.\n",
        "    example_selector=mmr_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"What's the opposite of...\",\n",
        "    suffix=\"Input: {adjective}\\nOutput:\",\n",
        "    input_variables=[\"adjective\"],\n",
        ")"
      ],
      "metadata": {
        "id": "DHJdQxLCejBf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mmr_prompt.format(adjective=\"buzzed\"))"
      ],
      "metadata": {
        "id": "vYwzdCSAeCoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54b173a-5bde-4107-bee7-d83fb6acb0f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What's the opposite of...\n",
            "\n",
            "Input: caffeinated\n",
            "Output: sleepy\n",
            "\n",
            "Input: crispy\n",
            "Output: soggy\n",
            "\n",
            "Input: buzzed\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select by n-gram overlap\n",
        "\n",
        "The `NGramOverlapExampleSelector` selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score.\n",
        "\n",
        "\n",
        "The ngram overlap score is a float between 0.0 and 1.0, inclusive.\n",
        "\n",
        "It will select and orders examples based on their n-gram overlap score with a given input.\n",
        "\n",
        "This overlap score is derived from the `sentence_bleu` score, a metric used in machine translation to evaluate translated sentences' quality.\n",
        "\n",
        "\n",
        "The selector provides the option to establish a threshold score. Examples with an n-gram overlap score less than or equal to this threshold are excluded. By default, the threshold is set at -1.0, which means it won't exclude any examples but will only reorder them.\n",
        "\n",
        "In the case of a threshold greater than 1.0, the select_examples function excludes all examples and returns an empty list.\n",
        "\n",
        "When the threshold is set to 0.0, select_examples sorts the examples based on their n-gram overlap score and excludes those that have no n-gram overlap with the input.\n",
        "\n"
      ],
      "metadata": {
        "id": "YcFO2scHfWKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `NGramOverlapExampleSelector` Overview\n",
        "\n",
        "\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **N-gram Overlap Score**:\n",
        "   - The `ngram_overlap_score` function calculates the n-gram overlap score between a source and an example using the `sentence_bleu` score.\n",
        "   - This function employs the `sentence_bleu` method with method1 smoothing and auto reweighting. The resulting score ranges between 0.0 and 1.0.\n",
        "\n",
        "2. **Attributes**:\n",
        "   - `examples`: Contains the list of examples the prompt template expects.\n",
        "   - `example_prompt`: The template used to format the examples.\n",
        "   - `threshold`: Determines when the algorithm stops selecting examples. It's set to -1.0 by default.\n",
        "\n",
        "3. **Methods**:\n",
        "\n",
        "   - **`add_example(example: Dict[str, str]) -> None`**:\n",
        "     - Adds a new example to the list.\n",
        "\n",
        "   - **`select_examples(input_variables: Dict[str, str]) -> List[dict]`**:\n",
        "     - Returns examples sorted by their n-gram overlap score with the input in descending order.\n",
        "     - Excludes examples with scores less than or equal to the threshold.\n",
        "\n",
        "### How It Works\n",
        "\n",
        "When you feed a set of input variables to `select_examples`, the selector computes each example's n-gram overlap score in its list. It then arranges these examples based on their scores in descending order. The method continues selecting examples until it either exhausts the examples or finds an example with a score below the set threshold.\n",
        "\n",
        "For example, with a threshold of 0.0, the selector will omit examples with no n-gram overlap with the input. If the threshold exceeds 1.0, it will exclude all examples.\n",
        "\n",
        "This class is handy when you want examples that share common n-grams with the input.\n"
      ],
      "metadata": {
        "id": "SHwoQxHppkQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector.ngram_overlap import NGramOverlapExampleSelector\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
      ],
      "metadata": {
        "id": "SnsccySfg-G5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of converting modern slang to Shakespearean English.\n",
        "examples = [\n",
        "    {\"input\": \"She's thirsty, ain't she?\", \"output\": \"She doth crave attention, doth she not?\"},\n",
        "    {\"input\": \"He's flexing those dollar bills.\", \"output\": \"He doth flaunt his gold coins, verily.\"},\n",
        "    {\"input\": \"That's some tea.\", \"output\": \"That's a tale most intriguing.\"},\n",
        "    {\"input\": \"It's lit!\", \"output\": \"It's a merry revel!\"},\n",
        "    {\"input\": \"Throwing shade\", \"output\": \"Casting aspersions\"},\n",
        "    {\"input\": \"No cap\", \"output\": \"In sooth, no falsehood\"},\n",
        "    {\"input\": \"Slide into the DMs\", \"output\": \"Venture into private missives\"},\n",
        "    {\"input\": \"Ghosted\", \"output\": \"Vanish'd like a spectre\"},\n",
        "    {\"input\": \"On fleek\", \"output\": \"In finest fettle\"},\n",
        "    {\"input\": \"Bae\", \"output\": \"Mine own beloved\"},\n",
        "    {\"input\": \"Squad goals\", \"output\": \"Band's aspirations\"},\n",
        "    {\"input\": \"YOLO\", \"output\": \"Thou liv'st but once\"},\n",
        "    {\"input\": \"FOMO\", \"output\": \"Fear of missing the revelry\"},\n",
        "    {\"input\": \"Slay, queen!\", \"output\": \"Triumph, fair maiden!\"},\n",
        "    {\"input\": \"I can't even\", \"output\": \"I am most perplexed\"},\n",
        "    {\"input\": \"It's a vibe\", \"output\": \"It's a certain jest and merriment\"},\n",
        "    {\"input\": \"Clap back\", \"output\": \"Retort with vigor\"},\n",
        "    {\"input\": \"Low key\", \"output\": \"In hushed tones\"},\n",
        "    {\"input\": \"High key\", \"output\": \"Loudly and proudly\"},\n",
        "    {\"input\": \"Spill the tea\", \"output\": \"Unveil the tale\"},\n",
        "    {\"input\": \"That's basic\", \"output\": \"That's most ordinary\"},\n",
        "    {\"input\": \"Savage\", \"output\": \"Ruthless, like a wild beast\"},\n",
        "    {\"input\": \"Mood\", \"output\": \"Mine current disposition\"},\n",
        "    {\"input\": \"Woke\", \"output\": \"Awakened to the truths\"},\n",
        "    {\"input\": \"Cancel culture\", \"output\": \"Banishment by the masses\"},\n",
        "    {\"input\": \"Netflix and chill\", \"output\": \"Watch plays and relax, mayhaps more\"},\n",
        "    {\"input\": \"Snack\", \"output\": \"A sight most pleasing\"},\n",
        "    {\"input\": \"Thicc\", \"output\": \"Full and robust\"},\n",
        "    {\"input\": \"Shook\", \"output\": \"Most startled and taken aback\"},\n",
        "    {\"input\": \"AF\", \"output\": \"In great measure\"}\n",
        "]"
      ],
      "metadata": {
        "id": "ANphIBTSg_Ss"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {'input': \"When someone says 'She's thirsty, ain't she?', they're implying she's seeking attention.\",\n",
        "     'output': 'When one remarks, \"She doth crave attention, doth she not?\", they suggest her desire for notice.'},\n",
        "\n",
        "    {'input': \"Saying 'He's flexing those dollar bills.' means he's showing off his wealth.\",\n",
        "     'output': 'To utter, \"He doth flaunt his gold coins, verily.\" is to say he parades his riches.'},\n",
        "\n",
        "    {'input': \"The phrase 'That's some tea.' refers to juicy gossip or interesting news.\",\n",
        "     'output': \"The saying, 'That's a tale most intriguing.' speaks of a story that piques interest.\"},\n",
        "\n",
        "    {'input': \"Exclaiming 'It's lit!' means the situation is exciting or fun.\",\n",
        "     'output': \"Declaring 'It's a merry revel!' signifies a joyous occasion.\"},\n",
        "    {'input': \"When someone uses 'Throwing shade', they're subtly expressing disapproval or contempt.\",\n",
        "     'output': 'When one says \"Casting aspersions\", they art discreetly showing disdain or scorn.'},\n",
        "\n",
        "    {'input': \"The term 'No cap' is used to emphasize that someone is not lying.\",\n",
        "     'output': 'The phrase \"In sooth, no falsehood\" is uttered to stress that one speaks the truth.'},\n",
        "\n",
        "    {'input': \"To 'Slide into the DMs' means to send someone a direct message, usually with romantic intent.\",\n",
        "     'output': 'To \"Venture into private missives\" is to send a personal letter, perchance with courtly designs.'},\n",
        "\n",
        "    {'input': \"If someone has 'Ghosted', they've suddenly cut off all communication without explanation.\",\n",
        "     'output': \"If one hath 'Vanish'd like a spectre', they've abruptly ceased all discourse without reason.\"},\n",
        "\n",
        "    {'input': \"The phrase 'On fleek' means that something is perfect or flawless.\",\n",
        "     'output': \"The saying 'In finest fettle' signifies that something is in impeccable condition.\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "WU7oZ_J-wOQ6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")"
      ],
      "metadata": {
        "id": "Mf862IlihAOy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = NGramOverlapExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    threshold=-1.0\n",
        ")\n",
        "\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Translate yon modern utterance into the tongue of the Bard\",\n",
        "    suffix=\"Modern: {sentence}\\nShakespearean:\",\n",
        "    input_variables=[\"sentence\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "NsR_SL-OhBAX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(sentence=\"Someone in a meeting mentioned they didn't want to be part of 'cancel culture'. What were they referring to?\"))"
      ],
      "metadata": {
        "id": "d3OGAMWBqYt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078b8d90-bf82-4cd6-a846-897aa3ee2a73"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate yon modern utterance into the tongue of the Bard\n",
            "\n",
            "Input: To 'Slide into the DMs' means to send someone a direct message, usually with romantic intent.\n",
            "Output: To \"Venture into private missives\" is to send a personal letter, perchance with courtly designs.\n",
            "\n",
            "Input: The phrase 'That's some tea.' refers to juicy gossip or interesting news.\n",
            "Output: The saying, 'That's a tale most intriguing.' speaks of a story that piques interest.\n",
            "\n",
            "Input: The term 'No cap' is used to emphasize that someone is not lying.\n",
            "Output: The phrase \"In sooth, no falsehood\" is uttered to stress that one speaks the truth.\n",
            "\n",
            "Input: When someone says 'She's thirsty, ain't she?', they're implying she's seeking attention.\n",
            "Output: When one remarks, \"She doth crave attention, doth she not?\", they suggest her desire for notice.\n",
            "\n",
            "Input: Saying 'He's flexing those dollar bills.' means he's showing off his wealth.\n",
            "Output: To utter, \"He doth flaunt his gold coins, verily.\" is to say he parades his riches.\n",
            "\n",
            "Input: Exclaiming 'It's lit!' means the situation is exciting or fun.\n",
            "Output: Declaring 'It's a merry revel!' signifies a joyous occasion.\n",
            "\n",
            "Input: When someone uses 'Throwing shade', they're subtly expressing disapproval or contempt.\n",
            "Output: When one says \"Casting aspersions\", they art discreetly showing disdain or scorn.\n",
            "\n",
            "Input: If someone has 'Ghosted', they've suddenly cut off all communication without explanation.\n",
            "Output: If one hath 'Vanish'd like a spectre', they've abruptly ceased all discourse without reason.\n",
            "\n",
            "Input: The phrase 'On fleek' means that something is perfect or flawless.\n",
            "Output: The saying 'In finest fettle' signifies that something is in impeccable condition.\n",
            "\n",
            "Modern: Someone in a meeting mentioned they didn't want to be part of 'cancel culture'. What were they referring to?\n",
            "Shakespearean:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector.threshold = 0.0\n",
        "print(dynamic_prompt.format(sentence=\"The maiden ghosted me post our rendezvous.\"))"
      ],
      "metadata": {
        "id": "D4CSG0M7sN56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcda6a2e-b824-468c-8701-5cf125940c40"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate yon modern utterance into the tongue of the Bard\n",
            "\n",
            "Input: The phrase 'On fleek' means that something is perfect or flawless.\n",
            "Output: The saying 'In finest fettle' signifies that something is in impeccable condition.\n",
            "\n",
            "Input: The phrase 'That's some tea.' refers to juicy gossip or interesting news.\n",
            "Output: The saying, 'That's a tale most intriguing.' speaks of a story that piques interest.\n",
            "\n",
            "Input: The term 'No cap' is used to emphasize that someone is not lying.\n",
            "Output: The phrase \"In sooth, no falsehood\" is uttered to stress that one speaks the truth.\n",
            "\n",
            "Modern: The maiden ghosted me post our rendezvous.\n",
            "Shakespearean:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `SemanticSimilarityExampleSelector` Overview\n",
        "\n",
        "This class is designed to select examples based on their semantic similarity to a given input. It's a type of example selector that leverages embeddings and a vector store to find the most semantically similar examples.\n",
        "\n",
        "### Attributes:\n",
        "\n",
        "1. **vectorstore**: An instance of `VectorStore` that contains information about the examples. This is where the embeddings of the examples are stored and queried.\n",
        "2. **k**: The number of examples to select. By default, it's set to 4.\n",
        "3. **example_keys**: Optional keys to filter the examples. If provided, only these keys will be considered when selecting examples.\n",
        "4. **input_keys**: Optional keys to filter the input. If provided, the search is based on these input variables instead of considering all variables.\n",
        "\n",
        "### Methods:\n",
        "\n",
        "1. **`add_example(example: Dict[str, str]) -> str`**:\n",
        "   - Adds a new example to the `vectorstore`.\n",
        "   - If `input_keys` are provided, it constructs a string representation of the example using only those keys. Otherwise, it uses all keys.\n",
        "   - The constructed string is then added to the `vectorstore`, and the ID of the added text is returned.\n",
        "\n",
        "2. **`select_examples(input_variables: Dict[str, str]) -> List[dict]`**:\n",
        "   - Selects examples based on their semantic similarity to the provided input variables.\n",
        "   - Constructs a query string from the input variables.\n",
        "   - Uses the `vectorstore` to search for the most similar examples to the query.\n",
        "   - Retrieves the actual examples from the metadata of the search results.\n",
        "   - If `example_keys` are provided, the returned examples are filtered to include only those keys.\n",
        "\n",
        "3. **`from_examples(...)`**:\n",
        "   - A class method that creates an instance of `SemanticSimilarityExampleSelector`.\n",
        "   - Initializes the `vectorstore` with the provided examples and embeddings.\n",
        "   - Returns an instance of `SemanticSimilarityExampleSelector` with the initialized `vectorstore`.\n",
        "\n",
        "### How It Works:\n",
        "\n",
        "- The class leverages embeddings to represent the semantic meaning of examples and input variables.\n",
        "- When you want to select examples that are semantically similar to a given input, you provide the input variables to the `select_examples` method.\n",
        "- The method then queries the `vectorstore` to find the most similar examples based on their embeddings.\n",
        "- The returned examples can be filtered based on `example_keys` if provided.\n"
      ],
      "metadata": {
        "id": "EcgnA9knw8fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples"
      ],
      "metadata": {
        "id": "EzVMuncBvsEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366c78a5-b80c-415c-c66f-45d2063eb250"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': \"When someone says 'She's thirsty, ain't she?', they're implying she's seeking attention.\",\n",
              "  'output': 'When one remarks, \"She doth crave attention, doth she not?\", they suggest her desire for notice.'},\n",
              " {'input': \"Saying 'He's flexing those dollar bills.' means he's showing off his wealth.\",\n",
              "  'output': 'To utter, \"He doth flaunt his gold coins, verily.\" is to say he parades his riches.'},\n",
              " {'input': \"The phrase 'That's some tea.' refers to juicy gossip or interesting news.\",\n",
              "  'output': \"The saying, 'That's a tale most intriguing.' speaks of a story that piques interest.\"},\n",
              " {'input': \"Exclaiming 'It's lit!' means the situation is exciting or fun.\",\n",
              "  'output': \"Declaring 'It's a merry revel!' signifies a joyous occasion.\"},\n",
              " {'input': \"When someone uses 'Throwing shade', they're subtly expressing disapproval or contempt.\",\n",
              "  'output': 'When one says \"Casting aspersions\", they art discreetly showing disdain or scorn.'},\n",
              " {'input': \"The term 'No cap' is used to emphasize that someone is not lying.\",\n",
              "  'output': 'The phrase \"In sooth, no falsehood\" is uttered to stress that one speaks the truth.'},\n",
              " {'input': \"To 'Slide into the DMs' means to send someone a direct message, usually with romantic intent.\",\n",
              "  'output': 'To \"Venture into private missives\" is to send a personal letter, perchance with courtly designs.'},\n",
              " {'input': \"If someone has 'Ghosted', they've suddenly cut off all communication without explanation.\",\n",
              "  'output': \"If one hath 'Vanish'd like a spectre', they've abruptly ceased all discourse without reason.\"},\n",
              " {'input': \"The phrase 'On fleek' means that something is perfect or flawless.\",\n",
              "  'output': \"The saying 'In finest fettle' signifies that something is in impeccable condition.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt"
      ],
      "metadata": {
        "id": "WrVh7XVHt0Sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3eb5214-34c6-4989-e33f-07374892ccd9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['input', 'output'], template='Input: {input}\\nOutput: {output}')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples,\n",
        "    OpenAIEmbeddings(),\n",
        "    FAISS,\n",
        "    k=4\n",
        ")\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # We provide an ExampleSelector instead of examples.\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Translate yon modern utterance into the tongue of the Bard\",\n",
        "    suffix=\"Modern: {sentence}\\nShakespearean:\",\n",
        "    input_variables=[\"sentence\"]\n",
        ")"
      ],
      "metadata": {
        "id": "VnKZHOjKt4Bu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(sentence=\"What does it mean when someone says they're 'Throwing shade' at another?\"))"
      ],
      "metadata": {
        "id": "rhPHqoiwt6m4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b488d9-4057-4589-897a-9a2fdce0ecea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate yon modern utterance into the tongue of the Bard\n",
            "\n",
            "Input: When someone says 'She's thirsty, ain't she?', they're implying she's seeking attention.\n",
            "Output: When one remarks, \"She doth crave attention, doth she not?\", they suggest her desire for notice.\n",
            "\n",
            "Input: When someone uses 'Throwing shade', they're subtly expressing disapproval or contempt.\n",
            "Output: When one says \"Casting aspersions\", they art discreetly showing disdain or scorn.\n",
            "\n",
            "Input: If someone has 'Ghosted', they've suddenly cut off all communication without explanation.\n",
            "Output: If one hath 'Vanish'd like a spectre', they've abruptly ceased all discourse without reason.\n",
            "\n",
            "Input: The term 'No cap' is used to emphasize that someone is not lying.\n",
            "Output: The phrase \"In sooth, no falsehood\" is uttered to stress that one speaks the truth.\n",
            "\n",
            "Input: To 'Slide into the DMs' means to send someone a direct message, usually with romantic intent.\n",
            "Output: To \"Venture into private missives\" is to send a personal letter, perchance with courtly designs.\n",
            "\n",
            "Modern: What does it mean when someone says they're 'Throwing shade' at another?\n",
            "Shakespearean:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(sentence=\"Someone told me my style was reminiscent of a 'snack'. What could they be hinting at?\"))"
      ],
      "metadata": {
        "id": "HL3pPfNVuYpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4cb99e0-70ba-4e2f-c1c7-45dbbeaa3bf2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate yon modern utterance into the tongue of the Bard\n",
            "\n",
            "Input: To 'Slide into the DMs' means to send someone a direct message, usually with romantic intent.\n",
            "Output: To \"Venture into private missives\" is to send a personal letter, perchance with courtly designs.\n",
            "\n",
            "Modern: Someone told me my style was reminiscent of a 'snack'. What could they be hinting at?\n",
            "Shakespearean:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(sentence=\"I overheard someone mention they felt 'shook' after watching a movie. What emotion were they expressing?\"))"
      ],
      "metadata": {
        "id": "2BQ5wWvNufY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489a6910-9f63-4381-bc49-5428a8fd1cdd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate yon modern utterance into the tongue of the Bard\n",
            "\n",
            "Input: To 'Slide into the DMs' means to send someone a direct message, usually with romantic intent.\n",
            "Output: To \"Venture into private missives\" is to send a personal letter, perchance with courtly designs.\n",
            "\n",
            "Input: When someone says 'She's thirsty, ain't she?', they're implying she's seeking attention.\n",
            "Output: When one remarks, \"She doth crave attention, doth she not?\", they suggest her desire for notice.\n",
            "\n",
            "Input: When someone uses 'Throwing shade', they're subtly expressing disapproval or contempt.\n",
            "Output: When one says \"Casting aspersions\", they art discreetly showing disdain or scorn.\n",
            "\n",
            "Input: The term 'No cap' is used to emphasize that someone is not lying.\n",
            "Output: The phrase \"In sooth, no falsehood\" is uttered to stress that one speaks the truth.\n",
            "\n",
            "Input: If someone has 'Ghosted', they've suddenly cut off all communication without explanation.\n",
            "Output: If one hath 'Vanish'd like a spectre', they've abruptly ceased all discourse without reason.\n",
            "\n",
            "Modern: I overheard someone mention they felt 'shook' after watching a movie. What emotion were they expressing?\n",
            "Shakespearean:\n"
          ]
        }
      ]
    }
  ]
}