{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHgv4EQ7hBxA",
        "outputId": "05162784-7b9e-4ccb-8f3b-36cae2ac5b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvaIosUctu5H"
      },
      "source": [
        "# FLARE\n",
        "\n",
        "### What is FLARE?\n",
        "\n",
        "[FLARE](https://arxiv.org/pdf/2305.06983.pdf) stands for `Forward-Looking Active REtrieval` augmented generation.  This is a method that was introduced in 2023 in a paper titled \"Active Retrieval Augmented Generation\" by Zhengbao Jiang, et al. The original implementation can be found in [the paper's GitHub repo](https://github.com/jzbjyb/FLARE), which the LangChain implementation draws heavy inspiration from.\n",
        "\n",
        "\n",
        "## 🚀 **Why FLARE Matters**\n",
        "\n",
        "- Enhances large language models by fetching info from external sources while generating content.\n",
        "\n",
        "- Tackles the issue of models making stuff up or being factually wrong.\n",
        "\n",
        "- Aims for outputs that are more accurate and well-informed.\n",
        "\n",
        "## 🔄 **FLARE's Approach:**\n",
        "- Starts by answering a question.\n",
        "\n",
        "- Looks up relevant documents when hitting \"uncertain\" tokens (based on log probs).\n",
        "\n",
        "- Uses these documents to keep generating.\n",
        "\n",
        "- Repeats the process until done.\n",
        "\n",
        "- Highlights uncertain tokens and forms questions for LLM to find answers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jzbjyb/FLARE/main/res/flare.gif\">\n",
        "\n",
        "## 🤔 **How FLARE Stands Out from Traditional RAG:**\n",
        "\n",
        "- Traditional RAG fetches info once based on the input, then generates an output.\n",
        "\n",
        "- Can be restrictive for long texts.\n",
        "\n",
        "- FLARE keeps deciding **when and what to fetch** during the generation.\n",
        "\n",
        "- Predicts the upcoming sentence to guide future content queries.\n",
        "\n",
        "- If a sentence has low-confidence tokens, it fetches relevant info and rewrites the sentence.\n",
        "\n",
        "\n",
        "### 💬 **FLARE Prompt Structure:**\n",
        "\n",
        "- Guides language models to create search queries during answer generation.\n",
        "\n",
        "- For example: \"The colors on the flag of Ghana mean... Red stands for [Search(Ghana flag red meaning)] the blood of martyrs...\"\n",
        "\n",
        "### 🔄 **Direct FLARE Method:**\n",
        "\n",
        "- Uses the next sentence generated by the language model for retrieval decisions.\n",
        "\n",
        "- Accepts the sentence if the model is confident.\n",
        "\n",
        "- If unsure, transforms the sentence into search queries to get more info, then revises the sentence.\n",
        "\n",
        "### 🛠 **Setting Up the FLARE Chain:**\n",
        "\n",
        "1. **LLM for Answers**: Needs to return log probabilities to spot uncertain tokens. Recommended: OpenAI wrapper (not ChatOpenAI wrapper, as it doesn't return logprobs).\n",
        "\n",
        "2. **LLM for Hypothetical Questions**: Can be any model. In this case, we use ChatOpenAI for its speed and cost-effectiveness.\n",
        "\n",
        "3. **Retriever**: Any retriever works. Here, the SERPER search engine is used for cost efficiency.\n",
        "\n",
        "#### 🔑 **Key Parameters in FLARE:**\n",
        "\n",
        "1. **`max_generation_len`**: Caps the number of tokens generated before checking for uncertainties.\n",
        "\n",
        "2. **`min_prob`**: Tokens generated with a probability below this threshold are marked as uncertain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ8pLgqvhedC",
        "outputId": "61fe1327-1a4e-4b86-de00-48a2598a4f85"
      },
      "outputs": [],
      "source": [
        "pip install langchain langchain-openai langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mEoXA-kQpdGz"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain.chains import FlareChain\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.callbacks.manager import AsyncCallbackManagerForRetrieverRun, CallbackManagerForRetrieverRun\n",
        "\n",
        "# We set this so we can see what exactly is going on\n",
        "from langchain.globals import set_verbose\n",
        "\n",
        "set_verbose(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoNiUTL8ahup"
      },
      "source": [
        "## Same pattern as we saw in the RAG lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rViYFL0Kh5XK",
        "outputId": "3ef7d039-b51f-40f1-8847-ccd8f773b805"
      },
      "outputs": [],
      "source": [
        "pip install arxiv frontend tools pymupdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X2B8uHxHjOmm"
      },
      "outputs": [],
      "source": [
        "!mkdir static"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QA126pbpu0UK"
      },
      "outputs": [],
      "source": [
        "# instantiate llm\n",
        "llm = ChatOpenAI(model=\"gpt-4-0125-preview\")\n",
        "\n",
        "#instantiate embeddings model\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# fetch docs from arxiv, in this case it's the FLARE paper\n",
        "docs = ArxivLoader(query=\"2305.06983\", load_max_docs=2).load()\n",
        "\n",
        "# instantiate text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=16)\n",
        "\n",
        "# split the document into chunks\n",
        "doc_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# create vector store consisting of embeddigs for document\n",
        "vector_store = FAISS.from_documents(doc_chunks, embedding=embeddings)\n",
        "\n",
        "vector_store_retriever = vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h0FJ6fsjLvw",
        "outputId": "a1fb9f87-7728-4cc0-cd3f-0bf525c55680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtSrDBBSa0i5"
      },
      "source": [
        "# Set up the FLARE chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4DZGInhUwgp_"
      },
      "outputs": [],
      "source": [
        "flare = FlareChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vector_store_retriever,\n",
        "    max_generation_len=1024,\n",
        "    min_prob=0.7,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE-f2-xma2q0"
      },
      "source": [
        "# Start generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ1oAcawL3RM",
        "outputId": "1fe27fdf-5dd1-4b55-f740-5eb7e8ca616c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new FlareChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What is FLARE?\n",
            ">>> RESPONSE: \u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What is FLARE?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "FLARE stands for Fast Light Attack and Reconnaissance Aircraft. It is a type of military aircraft designed for quick response and reconnaissance missions. It is often used for close air support, surveillance, and intelligence gathering.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" stands for Fast Light Attack and Reconnaissance\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What is FLARE?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "FLARE stands for Fast Light Attack and Reconnaissance Aircraft. It is a type of military aircraft designed for quick response and reconnaissance missions. It is often used for close air support, surveillance, and intelligence gathering.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" type of military aircraft designed for quick response and reconnaissance missions. It is often used for close air support, surveillance, and intelligence gathering.\" is:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What does the acronym FLARE expand to in the context of military aircraft?', 'What are the primary roles and capabilities of a FLARE in military operations?']\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "3\n",
            "FLARE: Forward-Looking Active\n",
            "REtrieval Augmented Generation\n",
            "Our intuition is that (1) LMs should only retrieve\n",
            "information when they do not have the necessary\n",
            "knowledge to avoid unnecessary or inappropriate\n",
            "\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.\n",
            "8\n",
            "Conclusion\n",
            "\n",
            "next sentence until reaches the end.\n",
            "FLARE is applicable to any existing LMs at\n",
            "inference time without additional training. Con-\n",
            "sidering the impressive performance achieved by\n",
            "GPT-3.5 (Ouyang et al., 2022) on a variety of\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.\n",
            "8\n",
            "Conclusion\n",
            "\n",
            "reported in Figure 4. FLARE outperforms all base-\n",
            "line on all tasks/datasets, indicating that FLARE\n",
            "is a generic method that can effectively retrieve\n",
            "additional information throughout the generation.\n",
            "Among various tasks, multihop QA shows the\n",
            "\n",
            "Previous-window ret.\n",
            "Forward-Looking Active REtrieval augmented generation (FLARE)\n",
            "Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for\n",
            ">>> USER INPUT: What is FLARE?\n",
            ">>> RESPONSE: \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response:  FLARE stands for Forward-Looking Active Retrieval augmented generation. It is a method that uses reinforcement learning or supervised training to retrieve information before generation, potentially improving the quality of the generated text. It has been shown to achieve superior or competitive performance on various knowledge-intensive generation tasks and datasets.\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What is FLARE?\n",
            ">>> RESPONSE:  FLARE stands for Forward-Looking Active Retrieval augmented generation. It is a method that uses reinforcement learning or supervised training to retrieve information before generation, potentially improving the quality of the generated text. It has been shown to achieve superior or competitive performance on various knowledge-intensive generation tasks and datasets.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: FLARE stands for Forward-Looking Active Retrieval augmented generation. It is a method that uses reinforcement learning or supervised training to retrieve information before generation, potentially improving the quality of the generated text. It has been shown to achieve superior or competitive performance on various knowledge-intensive generation tasks and datasets. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What is FLARE?\n",
            ">>> RESPONSE: FLARE stands for Forward-Looking Active Retrieval augmented generation. It is a method that uses reinforcement learning or supervised training to retrieve information before generation, potentially improving the quality of the generated text. It has been shown to achieve superior or competitive performance on various knowledge-intensive generation tasks and datasets. \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user_input': 'What is FLARE?',\n",
              " 'response': 'FLARE stands for Forward-Looking Active Retrieval augmented generation. It is a method that uses reinforcement learning or supervised training to retrieve information before generation, potentially improving the quality of the generated text. It has been shown to achieve superior or competitive performance on various knowledge-intensive generation tasks and datasets. \\n\\n'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flare.invoke(\"What is FLARE?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvaLunCFL4ug",
        "outputId": "28c29417-eb73-4eb8-9643-4e8919722d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new FlareChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: \u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" include data preprocessing, feature\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" model evaluation. Data preprocessing involves\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" formatting the data to make it suitable for analysis. Feature\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" and transforming relevant features from\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" using the extracted features to\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" Finally, model\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE:  \n",
            "The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" trained model on a separate dataset to assess\" is:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What do you call the sequential processes involved in a FLARE pipeline?', 'What are the initial steps involved in a FLARE pipeline for preparing the data for analysis?', \"What are the steps in a FLARE pipeline immediately following feature extraction and preceding the final assessment of the trained machine learning model's performance?\", 'What is involved in the initial step of data preprocessing in a FLARE pipeline?', 'What does the feature extraction step involve in the context of a FLARE pipeline, specifically after selecting relevant features?', 'What does the step of model training in a FLARE pipeline involve?', 'What is the stage in a FLARE pipeline that follows data preprocessing, feature extraction, and model training?', 'What is involved in the process of model evaluation in a FLARE pipeline?']\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: Iterative re-\n",
            "trieval and refinement has been studied in both\n",
            "text and code generation tasks (Peng et al., 2023;\n",
            "Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n",
            "et al., 2023). FLARE differs from these methods in\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.\n",
            "8\n",
            "Conclusion\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "\n",
            "input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n",
            "italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n",
            "\n",
            "reported in Figure 4. FLARE outperforms all base-\n",
            "line on all tasks/datasets, indicating that FLARE\n",
            "is a generic method that can effectively retrieve\n",
            "additional information throughout the generation.\n",
            "Among various tasks, multihop QA shows the\n",
            "\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.\n",
            "8\n",
            "Conclusion\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "next sentence until reaches the end.\n",
            "FLARE is applicable to any existing LMs at\n",
            "inference time without additional training. Con-\n",
            "sidering the impressive performance achieved by\n",
            "GPT-3.5 (Ouyang et al., 2022) on a variety of\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n",
            "italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "\n",
            "Iterative re-\n",
            "trieval and refinement has been studied in both\n",
            "text and code generation tasks (Peng et al., 2023;\n",
            "Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n",
            "et al., 2023). FLARE differs from these methods in\n",
            "\n",
            "Iterative re-\n",
            "trieval and refinement has been studied in both\n",
            "text and code generation tasks (Peng et al., 2023;\n",
            "Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n",
            "et al., 2023). FLARE differs from these methods in\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.\n",
            "8\n",
            "Conclusion\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.\n",
            "8\n",
            "Conclusion\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "\n",
            "next sentence until reaches the end.\n",
            "FLARE is applicable to any existing LMs at\n",
            "inference time without additional training. Con-\n",
            "sidering the impressive performance achieved by\n",
            "GPT-3.5 (Ouyang et al., 2022) on a variety of\n",
            "\n",
            "next sentence until reaches the end.\n",
            "FLARE is applicable to any existing LMs at\n",
            "inference time without additional training. Con-\n",
            "sidering the impressive performance achieved by\n",
            "GPT-3.5 (Ouyang et al., 2022) on a variety of\n",
            "\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.\n",
            "8\n",
            "Conclusion\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "\n",
            "Iterative re-\n",
            "trieval and refinement has been studied in both\n",
            "text and code generation tasks (Peng et al., 2023;\n",
            "Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n",
            "et al., 2023). FLARE differs from these methods in\n",
            "\n",
            "and UniEval (Zhong et al., 2022) which measures\n",
            "factual consistency.\n",
            "6\n",
            "Experimental Results\n",
            "We first report overall results across 4 tasks/datasets\n",
            "and compare the performance of FLARE with all\n",
            "the baselines introduced in section 4. We then\n",
            "\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1\n",
            "Introduction\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response:  The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality.\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE:  The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> EXISTING PARTIAL RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality.  FINISHED\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" FINISHED\" is:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What is the status indication used to denote the completion of the explanation of the steps in a FLARE pipeline?']\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: tent of future generations. FLARE outperforms\n",
            "this baseline, indicating that manual exemplar an-\n",
            "notation is not necessary for effective future-aware\n",
            "retrieval. The gap between FLAREinstruct and ques-\n",
            "tion decomposition is large, indicating that teaching\n",
            "\n",
            "input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n",
            "italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n",
            "\n",
            "reported in Figure 4. FLARE outperforms all base-\n",
            "line on all tasks/datasets, indicating that FLARE\n",
            "is a generic method that can effectively retrieve\n",
            "additional information throughout the generation.\n",
            "Among various tasks, multihop QA shows the\n",
            "\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003. We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE:  The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What are the steps in a FLARE pipeline?\n",
            ">>> RESPONSE: The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user_input': 'What are the steps in a FLARE pipeline?',\n",
              " 'response': 'The steps in a FLARE pipeline include iterative retrieval and refinement, generating temporary next sentences, and checking for low-probability tokens. FLARE can also be combined with a browser to potentially improve retrieval quality. '}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flare.invoke(\"What are the steps in a FLARE pipeline?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9K3HpCxMIJY",
        "outputId": "e6725fff-f48a-41e9-b81f-0e90f854bc18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new FlareChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: \u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> EXISTING PARTIAL RESPONSE:  A typical retrieval instruction in FLARE would look like a query that specifies the type of data you want to retrieve, the location of the data, and the format in which you want the data to be returned. For example, you could specify that you want to retrieve all customer records from a specific database in CSV format.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \"A typical retrieval instruction in FLARE would look like a query that specifies the type of data you want to retrieve, the location of the data\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> EXISTING PARTIAL RESPONSE:  A typical retrieval instruction in FLARE would look like a query that specifies the type of data you want to retrieve, the location of the data, and the format in which you want the data to be returned. For example, you could specify that you want to retrieve all customer records from a specific database in CSV format.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" the format in which you\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> EXISTING PARTIAL RESPONSE:  A typical retrieval instruction in FLARE would look like a query that specifies the type of data you want to retrieve, the location of the data, and the format in which you want the data to be returned. For example, you could specify that you want to retrieve all customer records from a specific database in CSV format.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" to be returned. For example, you could specify that you want\" is:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n",
            "\n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> EXISTING PARTIAL RESPONSE:  A typical retrieval instruction in FLARE would look like a query that specifies the type of data you want to retrieve, the location of the data, and the format in which you want the data to be returned. For example, you could specify that you want to retrieve all customer records from a specific database in CSV format.\n",
            "\n",
            "The question to which the answer is the term/entity/phrase \" customer records from a specific database in CSV format.\" is:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What are the key components of a retrieval instruction in the FLARE programming language?', 'What part of a FLARE retrieval instruction specifies how you want the data to be structured upon return?', 'What is included in the middle part of a typical retrieval instruction in FLARE, specifically between the details of the data you want to retrieve and the format you want it in?', 'What is an example of data you might want to retrieve using a FLARE query, specifying both the type of data and the desired format?']\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: Retriever\n",
            "$\n",
            "%$\n",
            "%%\n",
            "Figure 2: An illustration of forward-looking active re-\n",
            "trieval augmented generation with retrieval instructions\n",
            "(FLAREinstruct). It iteratively generates search queries\n",
            "(shown in gray italic) to retrieve relevant information to\n",
            "\n",
            "as FLAREdirect, which iteratively generates the next\n",
            "sentence to gain insight into the future topic, and\n",
            "if uncertain tokens are present, retrieves relevant\n",
            "documents to regenerate the next sentence.\n",
            "3.1\n",
            "FLARE with Retrieval Instructions\n",
            "\n",
            "Iterative re-\n",
            "trieval and refinement has been studied in both\n",
            "text and code generation tasks (Peng et al., 2023;\n",
            "Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n",
            "et al., 2023). FLARE differs from these methods in\n",
            "\n",
            "tive retrieval augmented generation, methods\n",
            "that actively decide when and what to retrieve\n",
            "across the course of the generation. We propose\n",
            "Forward-Looking Active REtrieval augmented\n",
            "generation (FLARE), a generic method which\n",
            "\n",
            "Retriever\n",
            "$\n",
            "%$\n",
            "%%\n",
            "Figure 2: An illustration of forward-looking active re-\n",
            "trieval augmented generation with retrieval instructions\n",
            "(FLAREinstruct). It iteratively generates search queries\n",
            "(shown in gray italic) to retrieve relevant information to\n",
            "\n",
            "as FLAREdirect, which iteratively generates the next\n",
            "sentence to gain insight into the future topic, and\n",
            "if uncertain tokens are present, retrieves relevant\n",
            "documents to regenerate the next sentence.\n",
            "3.1\n",
            "FLARE with Retrieval Instructions\n",
            "\n",
            "The structure of the prompt is shown in Prompt 3.1,\n",
            "and full details can be found in Prompt D.3.\n",
            "Prompt 3.1: retrieval instructions\n",
            "Skill 1. An instruction to guide LMs to generate search\n",
            "queries.\n",
            "Several search-related exemplars.\n",
            "Skill 2.\n",
            "\n",
            "3\n",
            "FLARE: Forward-Looking Active\n",
            "REtrieval Augmented Generation\n",
            "Our intuition is that (1) LMs should only retrieve\n",
            "information when they do not have the necessary\n",
            "knowledge to avoid unnecessary or inappropriate\n",
            "\n",
            "Retriever\n",
            "$\n",
            "%$\n",
            "%%\n",
            "Figure 2: An illustration of forward-looking active re-\n",
            "trieval augmented generation with retrieval instructions\n",
            "(FLAREinstruct). It iteratively generates search queries\n",
            "(shown in gray italic) to retrieve relevant information to\n",
            "\n",
            "as FLAREdirect, which iteratively generates the next\n",
            "sentence to gain insight into the future topic, and\n",
            "if uncertain tokens are present, retrieves relevant\n",
            "documents to regenerate the next sentence.\n",
            "3.1\n",
            "FLARE with Retrieval Instructions\n",
            "\n",
            "first method prompts the LM to generate retrieval\n",
            "queries when necessary while generating the an-\n",
            "swer using retrieval-encouraging instructions, de-\n",
            "noted as FLAREinstruct. The second method directly\n",
            "uses the LM’s generation as search queries, denoted\n",
            "\n",
            "retrieval, and (2) the retrieval queries should reflect\n",
            "the intents of future generations. We propose two\n",
            "forward-looking active retrieval augmented gener-\n",
            "ation (FLARE) methods to implement the active\n",
            "retrieval augmented generation framework. The\n",
            "\n",
            "Retriever\n",
            "$\n",
            "%$\n",
            "%%\n",
            "Figure 2: An illustration of forward-looking active re-\n",
            "trieval augmented generation with retrieval instructions\n",
            "(FLAREinstruct). It iteratively generates search queries\n",
            "(shown in gray italic) to retrieve relevant information to\n",
            "\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.\n",
            "8\n",
            "Conclusion\n",
            "\n",
            "Retrieved\n",
            "documents\n",
            "LM\n",
            "Generation\n",
            "$\n",
            "%$\n",
            "%%\n",
            "Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n",
            "\n",
            "retrieval, and (2) the retrieval queries should reflect\n",
            "the intents of future generations. We propose two\n",
            "forward-looking active retrieval augmented gener-\n",
            "ation (FLARE) methods to implement the active\n",
            "retrieval augmented generation framework. The\n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response:  A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations.\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE:  A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCurrent Response: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n",
            "\n",
            ">>> CONTEXT: \n",
            ">>> USER INPUT: What does a typical retrieval instruction look like in FLARE?\n",
            ">>> RESPONSE: A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user_input': 'What does a typical retrieval instruction look like in FLARE?',\n",
              " 'response': 'A typical retrieval instruction in FLARE is a prompt that guides the language model to generate search queries when necessary, while also encouraging retrieval during the generation process. This can be done using reinforcement learning or supervised training, and the retrieval queries should reflect the intents of future generations. '}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flare.invoke(\"What does a typical retrieval instruction look like in FLARE?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
