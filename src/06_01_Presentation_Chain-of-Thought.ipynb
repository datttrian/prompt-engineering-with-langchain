{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6611,"status":"ok","timestamp":1706679000028,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"ROv0ygONORNx","outputId":"dd015dc1-952d-42d5-8783-9770243f0327"},"outputs":[],"source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain\n","  Using cached langchain-0.2.2-py3-none-any.whl.metadata (13 kB)\n","Collecting PyYAML>=5.3 (from langchain)\n","  Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n","Collecting SQLAlchemy<3,>=1.4 (from langchain)\n","  Using cached SQLAlchemy-2.0.30-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n","Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n","  Using cached aiohttp-3.9.5-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n","Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n","  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n","Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n","  Using cached langchain_core-0.2.4-py3-none-any.whl.metadata (5.9 kB)\n","Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n","  Using cached langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Using cached langsmith-0.1.71-py3-none-any.whl.metadata (13 kB)\n","Collecting numpy<2,>=1 (from langchain)\n","  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n","Collecting pydantic<3,>=1 (from langchain)\n","  Using cached pydantic-2.7.3-py3-none-any.whl.metadata (108 kB)\n","Collecting requests<3,>=2 (from langchain)\n","  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n","  Using cached tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n","Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n","Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Using cached multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Using cached yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n","  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n","Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n","  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Using cached orjson-3.10.3-cp310-none-win_amd64.whl.metadata (50 kB)\n","Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n","  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n","Collecting pydantic-core==2.18.4 (from pydantic<3,>=1->langchain)\n","  Using cached pydantic_core-2.18.4-cp310-none-win_amd64.whl.metadata (6.7 kB)\n","Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.1)\n","Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n","  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n","Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n","  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n","  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n","Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n","  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n","Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n","  Using cached greenlet-3.0.3-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n","  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n","Using cached langchain-0.2.2-py3-none-any.whl (973 kB)\n","Using cached aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n","Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n","Using cached langchain_core-0.2.4-py3-none-any.whl (310 kB)\n","Using cached langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n","Using cached langsmith-0.1.71-py3-none-any.whl (124 kB)\n","Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n","Using cached pydantic-2.7.3-py3-none-any.whl (409 kB)\n","Using cached pydantic_core-2.18.4-cp310-none-win_amd64.whl (1.9 MB)\n","Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n","Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n","Using cached SQLAlchemy-2.0.30-cp310-cp310-win_amd64.whl (2.1 MB)\n","Using cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n","Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n","Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n","Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n","Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n","Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n","Using cached greenlet-3.0.3-cp310-cp310-win_amd64.whl (292 kB)\n","Using cached idna-3.7-py3-none-any.whl (66 kB)\n","Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Using cached multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n","Using cached orjson-3.10.3-cp310-none-win_amd64.whl (138 kB)\n","Using cached packaging-23.2-py3-none-any.whl (53 kB)\n","Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n","Using cached yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n","Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Installing collected packages: urllib3, tenacity, PyYAML, pydantic-core, packaging, orjson, numpy, multidict, jsonpointer, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, SQLAlchemy, requests, pydantic, jsonpatch, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 24.0\n","    Uninstalling packaging-24.0:\n","      Successfully uninstalled packaging-24.0\n","Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 attrs-23.2.0 certifi-2024.6.2 charset-normalizer-3.3.2 frozenlist-1.4.1 greenlet-3.0.3 idna-3.7 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.2 langchain-core-0.2.4 langchain-text-splitters-0.2.1 langsmith-0.1.71 multidict-6.0.5 numpy-1.26.4 orjson-3.10.3 packaging-23.2 pydantic-2.7.3 pydantic-core-2.18.4 requests-2.32.3 tenacity-8.3.0 urllib3-2.2.1 yarl-1.9.4\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install langchain"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":316,"status":"ok","timestamp":1706679002217,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"sq7OvPt1_2XX"},"outputs":[],"source":["from langchain.globals import set_verbose\n","\n","set_verbose(True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zEAWUvegOw27"},"source":["#ðŸ”— **Origins of CoT Prompting**\n","\n","- CoT was introduced by Wei et al. in their 2022 paper, [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://browse.arxiv.org/pdf/2201.11903.pdf).\n","\n","- It's a response to the need for better reasoning in large language models.\n","\n","ðŸ’¡ **What is CoT Prompting?**\n","- CoT stands for Chain of Thought Prompting.\n","\n","- It's about guiding language models through a step-by-step reasoning process.\n","\n","- The model shows its reasoning, forming a \"chain\" of thoughts, rather than just giving an answer.\n","\n","- Mimics human-like thought processes for complex tasks.\n","\n","\n","### ðŸŽ“ **Standard vs. CoT Prompting Example**\n","\n","<figure>\n","  <img src=\"https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1696369825-cot-example.png&w=3840&q=75\" alt=\"Image Description\" style=\"width:100%\">\n","  <figcaption>Examples of âŸ¨input, chain of thought, outputâŸ© triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted.</figcaption>\n","  <a href=\"https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1696369825-cot-example.png&w=3840&q=75\">Image Source</a>\n","</figure>\n","\n","- Standard: Direct answer with no reasoning (e.g., \"11\").\n","\n","- CoT: Detailed reasoning steps (e.g., \"Roger started with 5, added 6 from 2 cans, so 5 + 6 = 11\").\n","\n","### ðŸš€ **Usage of CoT Prompting**\n","\n","1. **Enhanced Reasoning**: Breaks down complex problems into simpler steps.\n","\n","2. **Combination with Few-Shot Prompting**: Uses examples to guide model responses, great for complex tasks.\n","\n","3. **Interpretability**: Offers a clear view of the model's thought process.\n","\n","4. **Applications**: Useful in arithmetic, commonsense reasoning, and symbolic tasks.\n","\n","### âœ¨ **Impact of CoT Prompting**\n","\n","- Improves accuracy and insightfulness in responses.\n","\n","- Focuses on reasoning and interpretability, especially in complex scenarios."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting datasets\n","  Using cached datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n","Collecting filelock (from datasets)\n","  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n","Collecting pyarrow>=12.0.0 (from datasets)\n","  Using cached pyarrow-16.1.0-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n","Collecting pyarrow-hotfix (from datasets)\n","  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting pandas (from datasets)\n","  Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl.metadata (19 kB)\n","Requirement already satisfied: requests>=2.32.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n","Collecting tqdm>=4.62.1 (from datasets)\n","  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n","Collecting xxhash (from datasets)\n","  Using cached xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets)\n","  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: aiohttp in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from datasets) (3.9.5)\n","Collecting huggingface-hub>=0.21.2 (from datasets)\n","  Using cached huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: packaging in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests>=2.32.1->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests>=2.32.1->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests>=2.32.1->datasets) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests>=2.32.1->datasets) (2024.6.2)\n","Requirement already satisfied: colorama in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n","Collecting pytz>=2020.1 (from pandas->datasets)\n","  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas->datasets)\n","  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: six>=1.5 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Using cached datasets-2.19.2-py3-none-any.whl (542 kB)\n","Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n","Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n","Using cached huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n","Using cached pyarrow-16.1.0-cp310-cp310-win_amd64.whl (25.9 MB)\n","Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n","Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n","Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n","Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Using cached xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n","Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n","Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n","Installing collected packages: pytz, xxhash, tzdata, tqdm, pyarrow-hotfix, pyarrow, fsspec, filelock, dill, pandas, multiprocess, huggingface-hub, datasets\n","Successfully installed datasets-2.19.2 dill-0.3.8 filelock-3.14.0 fsspec-2024.3.1 huggingface-hub-0.23.2 multiprocess-0.70.16 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pytz-2024.1 tqdm-4.66.4 tzdata-2024.1 xxhash-3.4.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install datasets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8ho8Ee15qQGw"},"source":["The code below downloads an dataset which has over [1.8 million Chain of Thought examples](https://huggingface.co/datasets/kaist-ai/CoT-Collection)."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"fCE2VgOPO_L4"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"kaist-ai/CoT-Collection\", split=\"train\", trust_remote_code=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":672,"status":"ok","timestamp":1706678952642,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"Z0DY6By96mqc","outputId":"91050859-80ad-4f88-ce64-01d72dc78ea7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","      <th>rationale</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Question: What about Neptune did NASA propose ...</td>\n","      <td>no</td>\n","      <td>The proposed mission to Neptune is not mention...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Choose your reply from the options at the end....</td>\n","      <td>yes</td>\n","      <td>The passage talks about the Tucson-Pima County...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>You are given an original reference as well as...</td>\n","      <td>1</td>\n","      <td>The system generated reference is grammaticall...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Is the premise \"A person on a beach in a green...</td>\n","      <td>yes</td>\n","      <td>The premise describes a situation where there ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Pick the option in line with common sense to a...</td>\n","      <td>C</td>\n","      <td>The rationale is: \"C, expected because it was ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              source target  \\\n","0  Question: What about Neptune did NASA propose ...     no   \n","1  Choose your reply from the options at the end....    yes   \n","2  You are given an original reference as well as...      1   \n","3  Is the premise \"A person on a beach in a green...    yes   \n","4  Pick the option in line with common sense to a...      C   \n","\n","                                           rationale  \n","0  The proposed mission to Neptune is not mention...  \n","1  The passage talks about the Tucson-Pima County...  \n","2  The system generated reference is grammaticall...  \n","3  The premise describes a situation where there ...  \n","4  The rationale is: \"C, expected because it was ...  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["dataset = dataset.remove_columns(['task', 'type'])\n","\n","dataset = dataset.shuffle(seed=42)\n","\n","dataset = dataset.select(range(10_000))\n","\n","dataset = dataset.to_pandas()\n","\n","dataset.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ATBYAzaXqfy1"},"source":["We'll sample just 10,000 of these Chain of Thought examples and save them as a list of dictionaries."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":311,"status":"ok","timestamp":1706678985658,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"duffkBFV69MV"},"outputs":[],"source":["selected_examples = dataset.to_dict(orient='records')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706678986795,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"kIFKE7Pd9YQj","outputId":"0b37eea0-e9dc-41b9-f583-02296169cc49"},"outputs":[{"data":{"text/plain":["{'source': 'Question: What about Neptune did NASA propose in 2003 in their \"Vision Missions Studies\"?\\n\\nIs However, there have been a couple of discussions to launch Neptune missions sooner. a good answer to this question?\\n\\nOPTIONS:\\n- yes\\n- no',\n"," 'target': 'no',\n"," 'rationale': 'The proposed mission to Neptune is not mentioned. The only mention of the planet in this excerpt is a note that there have been proposals for such missions, but they are very distant and probably will never happen.'}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["selected_examples[0]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain-community\n","  Using cached langchain_community-0.2.2-py3-none-any.whl.metadata (8.9 kB)\n","Requirement already satisfied: PyYAML>=5.3 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (3.9.5)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Using cached dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: langchain<0.3.0,>=0.2.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (0.2.2)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (0.2.4)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (0.1.71)\n","Requirement already satisfied: numpy<2,>=1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (1.26.4)\n","Requirement already satisfied: requests<3,>=2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-community) (8.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Using cached marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.1)\n","Requirement already satisfied: pydantic<3,>=1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n","Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n","Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.1)\n","Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n","Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.4)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Using cached langchain_community-0.2.2-py3-none-any.whl (2.2 MB)\n","Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n","Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n","Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n","Successfully installed dataclasses-json-0.6.6 langchain-community-0.2.2 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install langchain-community"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch\n","  Using cached torch-2.3.0-cp310-cp310-win_amd64.whl.metadata (26 kB)\n","Collecting transformers\n","  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n","Collecting sentence-transformers\n","  Using cached sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: filelock in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (4.12.1)\n","Collecting sympy (from torch)\n","  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n","Collecting networkx (from torch)\n","  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n","Collecting jinja2 (from torch)\n","  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: fsspec in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n","Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n","  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (0.23.2)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n","Collecting regex!=2019.12.17 (from transformers)\n","  Using cached regex-2024.5.15-cp310-cp310-win_amd64.whl.metadata (41 kB)\n","Requirement already satisfied: requests in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n","Collecting tokenizers<0.20,>=0.19 (from transformers)\n","  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n","Collecting safetensors>=0.4.1 (from transformers)\n","  Using cached safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (4.66.4)\n","Collecting scikit-learn (from sentence-transformers)\n","  Using cached scikit_learn-1.5.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n","Collecting scipy (from sentence-transformers)\n","  Using cached scipy-1.13.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n","Collecting Pillow (from sentence-transformers)\n","  Downloading pillow-10.3.0-cp310-cp310-win_amd64.whl.metadata (9.4 kB)\n","Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n","  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n","Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n","  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n","Requirement already satisfied: colorama in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Collecting MarkupSafe>=2.0 (from jinja2->torch)\n","  Using cached MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests->transformers) (2024.6.2)\n","Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n","  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n","Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n","  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n","Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n","  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Using cached torch-2.3.0-cp310-cp310-win_amd64.whl (159.8 MB)\n","Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n","Using cached sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n","Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n","Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n","Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n","Using cached regex-2024.5.15-cp310-cp310-win_amd64.whl (268 kB)\n","Using cached safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n","Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n","Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n","Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n","Downloading pillow-10.3.0-cp310-cp310-win_amd64.whl (2.5 MB)\n","   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n","   ---------- ----------------------------- 0.7/2.5 MB 13.9 MB/s eta 0:00:01\n","   ---------------------------------------- 2.5/2.5 MB 32.3 MB/s eta 0:00:00\n","Using cached scikit_learn-1.5.0-cp310-cp310-win_amd64.whl (11.0 MB)\n","Using cached scipy-1.13.1-cp310-cp310-win_amd64.whl (46.2 MB)\n","Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n","Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n","Using cached MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n","Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n","Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n","Installing collected packages: tbb, mpmath, intel-openmp, threadpoolctl, sympy, scipy, safetensors, regex, Pillow, networkx, mkl, MarkupSafe, joblib, scikit-learn, jinja2, torch, tokenizers, transformers, sentence-transformers\n","Successfully installed MarkupSafe-2.1.5 Pillow-10.3.0 intel-openmp-2021.4.0 jinja2-3.1.4 joblib-1.4.2 mkl-2021.4.0 mpmath-1.3.0 networkx-3.3 regex-2024.5.15 safetensors-0.4.3 scikit-learn-1.5.0 scipy-1.13.1 sentence-transformers-3.0.0 sympy-1.12.1 tbb-2021.12.0 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.3.0 transformers-4.41.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install torch transformers sentence-transformers"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (2.3.0)\n","Requirement already satisfied: transformers in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (4.41.2)\n","Requirement already satisfied: filelock in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (4.12.1)\n","Requirement already satisfied: sympy in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n","Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (0.23.2)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: intel-openmp==2021.* in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n","Requirement already satisfied: tbb==2021.* in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n","Requirement already satisfied: colorama in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\301369685\\desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install torch transformers"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ctCFgLemqll9"},"source":["For this example we'll use an open source embedding model from HuggingFace so we don't accumulate a huge bill for OpenAI"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3834,"status":"ok","timestamp":1706679095210,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"qmyv-uT-EmyS"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\301369685\\.cache\\huggingface\\hub\\models--BAAI--bge-base-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","c:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBAAI/bge-base-en-v1.5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m encode_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mnormalize_embeddings\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m} \u001b[39m# set True to compute cosine similarity\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m embeddings \u001b[39m=\u001b[39m HuggingFaceBgeEmbeddings(\n\u001b[0;32m      8\u001b[0m     model_name\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[0;32m      9\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m},\n\u001b[0;32m     10\u001b[0m     encode_kwargs\u001b[39m=\u001b[39;49mencode_kwargs\n\u001b[0;32m     11\u001b[0m )\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:268\u001b[0m, in \u001b[0;36mHuggingFaceBgeEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    263\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import sentence_transformers python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install sentence_transformers`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m sentence_transformers\u001b[39m.\u001b[39mSentenceTransformer(\n\u001b[0;32m    269\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name, cache_folder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_folder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_kwargs\n\u001b[0;32m    270\u001b[0m )\n\u001b[0;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m-zh\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name:\n\u001b[0;32m    272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_instruction \u001b[39m=\u001b[39m DEFAULT_QUERY_BGE_INSTRUCTION_ZH\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:315\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[0;32m    311\u001b[0m     modules \u001b[39m=\u001b[39m OrderedDict([(\u001b[39mstr\u001b[39m(idx), module) \u001b[39mfor\u001b[39;00m idx, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(modules)])\n\u001b[0;32m    313\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(modules)\n\u001b[1;32m--> 315\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_hpu_graph_enabled \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_prompt_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_prompt_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompts:\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[1;31m[... skipping similar frames: Module._apply at line 779 (1 times)]\u001b[0m\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[0;32m   1160\u001b[0m         device,\n\u001b[0;32m   1161\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1162\u001b[0m         non_blocking,\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[0;32m   1164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n","File \u001b[1;32mc:\\Users\\301369685\\Desktop\\prompt-engineering-with-langchain\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n","\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","\n","model_name = \"BAAI/bge-base-en-v1.5\"\n","\n","encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n","\n","embeddings = HuggingFaceBgeEmbeddings(\n","    model_name=model_name,\n","    model_kwargs={'device': 'cuda'},\n","    encode_kwargs=encode_kwargs\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aU69WA8pq0wb"},"source":["To set up a Chain of Thought prompt you will use a `FewShotPromptTemplate` as well as an example selector.\n","\n","In this example you'll use `MaxMarginalRelevanceExampleSelector`, but you can use any of the example selectors you learned about before."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":351,"status":"ok","timestamp":1706679404593,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"6YLFBPqMFId7"},"outputs":[],"source":["from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n","\n","from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n","\n","from langchain_community.vectorstores import FAISS"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":347,"status":"ok","timestamp":1706679442334,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"zYhYHn7NF4n_"},"outputs":[],"source":["prefix = \"Consider the following as examples of how to reason:\"\n","\n","examples_template = \"\"\"Query: {source}\n","\n","Rationale: {rationale}\n","\n","Response: {target}\n","\"\"\"\n","\n","suffix = \"\"\"Using a similar reasoning approach, answer the users question which is delimited by triple backticks.\n","\n","User question: ```{input}```\n","\n","Take a deep breath, break down the user's query step-by-step, and provide a clear chain of thought in your response.\"\n","\"\"\""]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":364,"status":"ok","timestamp":1706679453143,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"nXD2hFzNH9hL"},"outputs":[],"source":["examples_prompt = PromptTemplate(\n","    input_variables=[\"source\", \"rationale\", \"target\"],\n","    template=examples_template\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vXuPx89GtMLv"},"source":["There's a large number of examples to embed and add to the vector store. The below cell will take a few minutes to run. It took me ~3 minutes using the free T4 GPU provided on Google colab."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":186888,"status":"ok","timestamp":1706679667969,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"5SA1Tst3IO6J"},"outputs":[],"source":["example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n","    # The list of examples available to select from.\n","    selected_examples,\n","    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n","    embeddings,\n","    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n","    FAISS,\n","    # The number of examples to produce.\n","    k=7,\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1706679667970,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"cuFE8t32IX7S"},"outputs":[],"source":["mmr_prompt = FewShotPromptTemplate(\n","    example_selector=example_selector,\n","    example_prompt=examples_prompt,\n","    prefix=prefix,\n","    suffix=suffix,\n","    input_variables=[\"input\"]\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706679667970,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"shEayc6UI0o1"},"outputs":[],"source":["query = \"\"\"Here's a short story: A lifter uses a barbell, but moves a jump rope\\\n","in a wider arc. The object likely to travel further is (A) the jump rope (B) the\\\n","barbell.\n","\n","What is the most sensical answer between \"a jump rope\" and \"a barbell\"?\n","\"\"\""]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706679667970,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"KfA-c4gvJM9Q","outputId":"300a3d1d-cae1-409c-acba-0f31100b73d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Consider the following as examples of how to reason:\n","\n","Query: In this task, you will be shown a short story with a beginning, two potential middles, and an ending. Your job is to choose the middle statement that makes the story coherent / plausible by writing \"1\" or \"2\" in the output. If both sentences are plausible, pick the one that makes most sense.\n","\n","Beginning: I was in gymnastics class one day. Middle 1: I had stumbled on a vaulting horse and injured. Middle 2: I  had my best performer on a horse. Ending: This event ended my gymnastics career.\n","\n","Rationale: Middle 1 indicates that I had stumbled on a vaulting horse, which resulted in an injury. This event ended my gymnastics career. Middle 2 does not make sense because a gymnast's best performance would not be on the vaulting horse, but rather on other apparatuses such as the balance beam or uneven bars.\n","\n","Response: 1\n","\n","\n","Query: I am testing my students' logic.\n","What is the answer they should choose between \"Steel cable\" and \"Rubber cable\"?\n","Logic test: If you are going to bungee jump it is better to use a rubber cable than a steel cable because the steel cable is less flexible and (A) more breakable (B) less breakable\n","\n","Rationale: Here's the reason why they should choose \"Steel cable\":\n","- The steel cable is less flexible and more breakable.\n","\n","Response: Steel cable\n","\n","\n","Query: I am testing my students' logic.\n","What is the answer they should choose between \"Milo\" and \"Jimbo\"?\n","Logic test: If Jimbo is lifting weights and Milo is sitting quietly, which person is sweating less? (A) Jimbo (B) Milo\n","\n","Rationale: Jimbo is lifting weights while Milo is sitting quietly. This means that Jimbo will be expending more energy and thus sweating much more than Milo, who's doing nothing or next to nothing at the moment.\n","As a result of this, it seems as though Answer (B) \"Milo\" would be incorrect.\n","\n","Response: Jimbo\n","\n","\n","Query: Two analogies that relate actions to the tools used to perform the action is given in the form \"A : B. C : ?\". \"A : B\" relates action A to tool B. Your task is to replace the question mark (?) with the appropriate tool for the given action C, following the \"A : B\" relation.\n","\n","hit : hammer. crack : ?\n","\n","Rationale: A hammer is used to hit something, while a nutcracker is used to crack nuts.\n","\n","Response: nutcracker\n","\n","\n","Query: The further objects are away from each other, the smaller the gravitational force.\n","\n","Having read the above passage, choose the right answer to the following question (choices are increase or decrease ):\n","\n","\n","If Jim is playing with two objects and he moves them closer together to each other, what will happen to the gravitational force between the two objects increase or decrease?\n","\n","Rationale: The passage states that the further objects are away from each other, the smaller the gravitational force between them. If Jim were to move two objects closer together they would be less far apart and therefore have a greater gravitational force than if he moved them further apart, making \"increase\" the correct answer.\n","\n","Response: increase\n","\n","\n","Query: Here's a short story: Kent is practicing for roller hockey. His team is practicing on the gym's wood floor. He notices that the puck gets less heat when he hits it in the gym than when he is practicing alone at his home on the concrete patio. The puck will travel farther when Kent hits it across the (A) wood floor (B) concrete..\n","\n","What is the most sensical answer between \"wood floor\" and  \"concrete\"?\n","\n","Rationale: In the given context, Kent is practicing for roller hockey. The text mentions that as he practices on a wood floor in the gym, he notices that his puck travels farther than when it moves across concrete at home. This suggests that a wooden surface provides friction which results in less heat and allows him to hit the puck further than if there were no resistance from the surface. \n","Given this information, \"wood floor\" would be most sensical answer because it has more friction and therefore reduces heat produced by hitting the puck compared with \"concrete\".\n","\n","Response: wood floor\n","\n","\n","Query: Write a brief sentence.\n","\n","Rationale: Here's a brief sentence: A person is jumping over a pool.\n","\n","Response: A person is jumping over a pool.\n","\n","\n","Using a similar reasoning approach, answer the users question which is delimited by triple backticks.\n","\n","User question: ```Here's a short story: A lifter uses a barbell, but moves a jump ropein a wider arc. The object likely to travel further is (A) the jump rope (B) thebarbell.\n","\n","What is the most sensical answer between \"a jump rope\" and \"a barbell\"?\n","```\n","\n","Take a deep breath, break down the user's query step-by-step, and provide a clear chain of thought in your response.\"\n","\n"]}],"source":["print(mmr_prompt.format(input=query))"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17532,"status":"ok","timestamp":1706679778282,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"KFw8vxigJTjW","outputId":"ce515fde-f96a-45b8-eb8f-0728907b3161"},"outputs":[{"name":"stdout","output_type":"stream","text":["Given the user's query, let's analyze the information provided:\n","\n","- The lifter is using two different objects: a barbell and a jump rope.\n","- The jump rope is being moved in a wider arc compared to how the barbell is used.\n","\n","To determine which object is likely to travel further, we need to consider the nature of the objects and how they are being used.\n","\n","A barbell is a heavy piece of equipment primarily used for strength training. Its weight and design are not conducive to being \"thrown\" or \"traveling\" far distances, especially when compared to lighter objects. The use of a barbell typically involves lifting and lowering rather than throwing or swinging in wide arcs.\n","\n","On the other hand, a jump rope is lightweight and designed to be swung around the body. Moving a jump rope in a wider arc increases its momentum, making it capable of traveling a greater distance if released. The wider the arc, the more momentum is generated, which could potentially cause the jump rope to travel further if it were to be let go.\n","\n","Given these considerations, the object likely to travel further when moved in a wider arc is the jump rope. This is because its lightweight design and the action of swinging it in a wider arc contribute to greater momentum, which would allow it to cover more distance compared to a heavy, less mobile barbell.\n","\n","Response: a jump rope"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0.0)\n","\n","llm_chain = mmr_prompt | llm | StrOutputParser()\n","\n","for chunk in llm_chain.stream({\"input\":query}):\n","  print(chunk, end=\"\", flush=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1706679798740,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"FftbmTl6LgQ-"},"outputs":[],"source":["query = \"\"\"\n","Given the fact that: Inhaling, or breathing in, increases the size of the chest, \\\n"," which decreases air pressure inside the lungs. Answer the question: If Mona is \\\n"," done with a race and her chest contracts, what happens to the amount of air \\\n"," pressure in her lungs increases or decreases?\n","\"\"\""]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21125,"status":"ok","timestamp":1706679821198,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"OZrMB0i0Lzif","outputId":"8f82828f-309a-4f05-acde-7a7500b1240e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Given the fact that inhaling, or breathing in, increases the size of the chest, which decreases air pressure inside the lungs, we can infer the opposite actionâ€”chest contracting (which occurs when exhaling)â€”would have the opposite effect on air pressure inside the lungs.\n","\n","When Mona's chest contracts after she is done with a race, this action is akin to exhaling, where the volume of the chest cavity decreases. According to Boyle's law, for a given mass of gas at constant temperature, the volume of the gas is inversely proportional to its pressure. Therefore, when the volume of the chest (and thus the lungs) decreases due to contraction, the air pressure inside the lungs must increase.\n","\n","Rationale: The initial context provided states that inhaling decreases air pressure inside the lungs due to an increase in chest size. By logical extension, when the chest contracts (a decrease in chest size), the air pressure inside the lungs must increase as the volume of space for the air decreases.\n","\n","Response: increases"]}],"source":["for chunk in llm_chain.stream({\"input\":query}):\n","  print(chunk, end=\"\", flush=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJFjvwxWCwvX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNUpicQNfihPPdnwto66QtM","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}
