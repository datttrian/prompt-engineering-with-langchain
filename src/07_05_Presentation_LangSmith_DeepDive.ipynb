{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith Deep Dive\n",
        "\n",
        "This notebook is co-authored with my friends at [AI MakerSpace](https://aimakerspace.io/). Check out their [YouTube channel](https://www.youtube.com/@AI-Makerspace/featured) for, hands down, the best educational content for all things LLMs.\n",
        "\n",
        "Be sure to connect with [Chris Alexiuk](https://ca.linkedin.com/in/csalexiuk) and [Greg Loughnane](https://www.linkedin.com/in/gregloughnane) on LinkedIn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Depenedencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "b22327f0-acba-4869-dbfa-4ad02dec340e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NpPwk1YAgl"
      },
      "source": [
        "## Basic RAG Chain\n",
        "\n",
        "Now we'll set up our basic RAG chain, first up we need a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUWXhsNVYLTA"
      },
      "source": [
        "### OpenAI Model\n",
        "\n",
        "\n",
        "We'll use OpenAI's `gpt-3.5-turbo` model to ensure we can use a stronger model for decent evaluation later!\n",
        "\n",
        "Notice that we can tag our resources - this will help us be able to keep track of which resources were used where later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s6DNVa52MMZ",
        "outputId": "bc846808-f508-4777-d384-7636027aa9d8"
      },
      "outputs": [],
      "source": [
        "pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CSgK6jgw_tI3"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiagvgVDYTPn"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ntIqnv4cA5gR"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDO0XJqbYabb"
      },
      "source": [
        "### SiteMap Loader\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtTsO5dJ2WVd",
        "outputId": "2f1a2402-1efc-4c59-f756-6cc410f29441"
      },
      "outputs": [],
      "source": [
        "pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAS3QBQSARiw",
        "outputId": "b181d5c1-6888-4314-9132-9566cfbaa8ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Fetching pages: 100%|##########| 214/214 [00:22<00:00,  9.48it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0lnrY14zC0G",
        "outputId": "75cf97a5-4a25-4804-b838-e6b5c118a78e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"\\n\\n\\nHow Dosu Used LangSmith to Achieve a 30% Accuracy Improvement with No Prompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Dosu Used LangSmith to Achieve a 30% Accuracy Improvement with No Prompt Engineering\\n\\n6 min read\\nMay 2, 2024\\n\\n\\n\\n\\n\\nEditor's Note: the following is authored by Devin Stein, CEO of Dosu.\\xa0In this blog we walk through how Dosu uses LangSmith to improve the performance of their application - with NO prompt engineering. Rather, they collected feedback from their users, transformed that into few shot examples, and then fed that back into their application.This is a relatively simple and general technique that can lead to automatic performance improvements. We've written up a LangSmith cookbook to let anyone get started with continual in-context learning for classification! If learning from videos is more your style, check out our YouTube walkthrough here.YouTube WalkthroughLangSmith cookbookThere are many techniques for ‚Äúteaching‚Äù LLMs to improve their performance on specific tasks. The most common are:Prompt EngineeringFine-TuningIn-Context LearningPrompt engineering is the easiest and most common approach to help LLMs learn, but Dosu takes a different approach. Our team is not using prompt engineering, and we see significantly better results.Dosu is an Engineering Teammate who LearnsDosu is an engineering teammate that acts as the first line of defense for ad-hoc engineering requests, protecting engineers from unnecessary interruptions and unblocking GTM teams. We intentionally use the word ‚Äúteammate‚Äù rather than copilot or assistant because, like a teammate, Dosu should learn the nuances and workflows specific to your organization.If you haven‚Äôt heard of Dosu, you can check out our previous blog post or see these examples. At its core, Dosu automates the work engineers don‚Äôt want to do. A simple example of this is labeling. Very few engineers want to spend their time managing labels on tickets and PRs (if you‚Äôre reading this and you DO like this..we are hiring üòâ), however, having consistent, high-quality labels is important! Labels allow engineering teams to search, understand, and optimize where they are spending their time. If you‚Äôre skeptical, watch this recent KubeCon talk by the legendary Kubernetes maintainer MyBobbyTables, explaining why labeling is critical to engineering productivity.Dosu automatically labels tickets for you, so you get all the benefits without the work. Sounds great, right? But to be useful Dosu has to be correct. Incorrect labels can cause more work than having no labels at all. On the surface, labeling seems like a straightforward task; however, in practice, labels are often subjective and unique to an organization. For example, the enhancement label at LangChain is about a net-new library feature or integration, whereas, the same enhancement label at Dosu is exclusively for improvements to existing functionality. To do its job, Dosu needs to learn the meaning and rules for labels specific to each organization. So how can we teach Dosu to do this?Prompt Engineering within Products Leads to Poor UXAlthough prompt engineering can make a big difference in performance for LLMs, Dosu is more than an LLM. It‚Äôs a product. The magic of LLMs comes from those moments when they ‚Äújust work.‚Äù We think putting the burden of prompt engineering on users reduces that magic and leads to an unreliable product experience. To be more specific:Prompts are finicky. We cannot guarantee a reliable product experience if the product depends on a user‚Äôs ability to prompt engineer.Prompts are model-dependent. We want Dosu to use the best LLM for any given task. We don‚Äôt want internal LLM changes to break a prompt that a user spent hours crafting.Prompts are static. Organizations are constantly changing. Hard-coded logic in prompts can become stale and incorrect quickly.Fine-tuned Models are Complex to Manage and Susceptible to Data DriftIf prompt engineering is off the table, what about fine-tuning? Dosu has enough traffic that collecting a fine-tuning dataset is relatively straightforward, but fine-tuning comes with a few deal-breaking drawbacks:Fine-tuned models are complex to manage. If we need to fine-tune models for N customers, we have N different models that we need to serve, retrain, and monitor. This is solvable but time-consuming.Fine-tuned models are static. Similar to prompts, fine-tuned models are fixed to a point in time. Organizations change, causing fine-tuned model performance to degrade in unexpected ways due to data drift.\\xa0It‚Äôs important to highlight that these trade-offs are specifically for tasks where the expected output varies based on each organization. For tasks with the same expected output across all organizations, like input classification, fine-tuning is a perfect solution to optimize performance.Static In-Context Learning is also Susceptible to Data DriftThat leaves us with in-context learning, also known as few-shot learning. As a refresher, in-context learning is a technique where the LLM prompt includes example input/output pairs for a given task. In-context learning is simple but effective. It can be so effective that libraries like DSPy, which finds the optimal few-shot examples for you, can improve performance by as much as 65%.From a product perspective, there is a lot to like about in-context learning. When Dosu is wrong, users often correct it. This naturally creates an input/output example for in-context learning, meaning users can teach Dosu without knowing anything about LLMs.\\xa0Operationally, in-context learning reduces prompt complexity and decreases switching costs to change LLMs. By relying on examples to demonstrate common failure modes and edge cases to the LLM, we avoid crafting brittle, complex prompts that are optimized for a particular LLM.Although in-context learning gets us what we want from a product perspective, most references to in-context learning in papers rely on static examples and are still susceptible to data-drift. As discussed, organizations are dynamic, and we need Dosu to adapt to their changes.Continual In-Context Learning is Simple and EffectiveAn elegant part of in-context learning is there is only one variable to adjust: the examples.To teach Dosu about the particulars of an organization, all we need to do is pick the optimal examples for that organization for a given task at a given time.Before we can choose the best examples, we need to collect them. As mentioned earlier, when users correct Dosu, we save their corrections as an example for that task and then associate it with the user‚Äôs organization. We store all of these examples in a database that we refer to as an example store (akin to a traditional ML feature store).Now, whenever Dosu is going to complete a task, we can search our example store to find the most relevant examples. This transforms our learning problem into a retrieval problem, similar to what we already do in RAG.\\xa0And, that‚Äôs it. The final continual in-context learning flow is conceptually simple:Collect corrections from users and save them to an example storeAt inference time, search the example store and try to find the optimal examples for the current inputRepeatThe end result gives us exactly what we were looking for: a natural way for Dosu to learn about an organization and adapt to its changes over time.Implementing Continual Learning with LangSmith\\xa0At Dosu, we‚Äôve been long-time users of LangSmith. When we decided on continual in-context learning as the direction for teaching Dosu about organizations, we looked to see if we could implement it with existing tools. Fortunately, LangSmith has all the building blocks to easily implement continual learning.\\xa0For collecting corrections, LangSmith allows you to attach a correction as feedback to a run. And for our example store, we can rely on LangSmith‚Äôs Datasets. To insert examples into LangSmith, we can either use rules or insert them via the Datasets API.üí°If you want to try this out for yourself, check out our cookbook here which walks through this exact taskBuilding the World‚Äôs Best GitHub Auto LabelerWe wanted to put our new continual in-context learning pipeline to the test. The hardest part of the pipeline is collecting corrections from users. Auto labeling was an ideal first candidate because there is a clear correct answer, which makes collecting corrections simple.Every time a user either adds a label that Dosu missed or removes one of the labels Dosu added, we have a webhook that saves it as a correction on the run in LangSmith. This triggers a rule that automatically inserts the correction as an example to our LangSmith dataset with all the relevant metadata, such as the related organization ID.Now, the next time Dosu labels an issue or PR, we do a similarity search across all recent examples for the current input and organization. We take the top examples, inject them into the auto-label prompt, and run inference.We released auto-labeling with continual learning into production a month ago, and the results have been awesome. Dosu‚Äôs auto-labeling accuracy increased by over 30%. It‚Äôs the best GitHub auto-labeler that exists as far as we know. But more importantly, our customers love it.Continual Learning is the Future of AgentsContinual Learning creates a magical product experience. It gives power to end-users to tailor Dosu to meet their needs, and it correlates the time you invest in Dosu to the value you get out.With continual learning, Dosu can actually feel like a teammate. Dosu might make mistakes, but we can make sure Dosu, like a teammate, learns from those mistakes and doesn‚Äôt make them again.Auto-labeling is only one example of where we are incorporating continual learning. We are actively exploring other ways to integrate continual learning into retrieval, answer generation, and Dosu‚Äôs many other tasks.If you‚Äôre interested in trying out Dosu to improve engineering velocity or want to help us build self-learning agentic systems, reach out to hi@dosu.devIf you want to try this out for yourself with LangSmith, check out our cookbook here or our YouTube walkthrough here.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            ¬© LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://blog.langchain.dev/dosu-langsmith-no-prompt-eng/', 'loc': 'https://blog.langchain.dev/dosu-langsmith-no-prompt-eng/', 'lastmod': '2024-06-07T19:04:58.000Z'})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_s_x87H0BYmn",
        "outputId": "64a4d9c5-6667-444a-99ae-80b76899000a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://blog.langchain.dev/dosu-langsmith-no-prompt-eng/'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F79PdFcaYfBL"
      },
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "We're going to use a relatively naive text splitting strategy today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NmCdYTTTA4du"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "split_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 256,\n",
        "    chunk_overlap = 16\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLA5-LNBBVM-",
        "outputId": "f4ddadd7-4995-475f-b082-cbe36a96a484"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2569"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkFUuWVOzRXd",
        "outputId": "d332a6d1-d373-41eb-f108-5808a7bd2a24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='are useful when you have a dataset with multiple categories to evaluate separately. This allows you to test new use cases by adding examples to a separate split to test, while preserving your evaluation workflow.In addition to splits, you can speed up finding relevant information with the following actions for your dataset examples:Clone examples to another datasetEdit metadata directly in the UISearch for specific examplesWalk through an example of how to use dataset splits in this video.üîÅ\\xa0Repetitions to build confidence in your experiment resultsYou can now run multiple repetitions of your experiment in LangSmith. This helps smooth out noise from variability introduced by your application or from your LLM-as-a-judge evaluator, so you can build more confidence in the results of your experiment.In this video, learn how to evaluate on a dataset with repetitions. You can check the mean score across N repetitions, and also compare the outputs for variability across repetitions.üîß\\xa0Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAGLangSmith‚Äôs online evaluator (LLM-as-judge) automatically runs on production traces with a customizable prompt and automation rules. Our new', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-05-31T16:42:05.000Z'})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[42]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUsEc07iYnwj"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "We'll be leveraging OpenAI's Embeddings Models today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QVhMN0aaBrsM"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "base_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLoO_2MaY0TS"
      },
      "source": [
        "### FAISS VectorStore Retriever\n",
        "\n",
        "Now we can use a FAISS VectorStore to embed and store our documents and then convert it to a retriever so it can be used in our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsgyEyBB2_vW",
        "outputId": "53bc408d-cfc4-4e31-f2ba-f38242990ffd"
      },
      "outputs": [],
      "source": [
        "pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nBTK9kSFBWM1"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(split_documents, base_embeddings_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZpwDxlniCJRu"
      },
      "outputs": [],
      "source": [
        "base_retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GPhHPAY5yG"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "All we have left is a prompt template, which we'll create here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YAU74penCNmR"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmT5VyLmZAAK"
      },
      "source": [
        "### LCEL Chain\n",
        "\n",
        "Now that we have:\n",
        "\n",
        "- Embeddings Model\n",
        "- Generation Model\n",
        "- Retriever\n",
        "- Prompt\n",
        "\n",
        "We're ready to build our LCEL chain!\n",
        "\n",
        "Keep in mind that we're returning our source documents with our queries - while this isn't necessary, it's a great thing to get into the habit of doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pqVAsUc_Cp-7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "base_rag_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fNjMoS-ZVo5"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dq9rCScDfBE",
        "outputId": "b2b110e7-a355-468e-9501-81e44e709cd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': 'A good way to evaluate agents is through assisted evaluation, which can help guide you to the most interesting datapoint to look at. Evaluating LLM output using LLMs is not perfect, but it is currently considered the best available solution and is seen as promising in the long run.',\n",
              " 'context': [Document(page_content='Agents may be the ‚Äúkiller‚Äù LLM app, but building and evaluating agents is hard. Function calling is a key skill for effective tool use, but there aren‚Äôt many good benchmarks for measuring function calling performance. Today, we are excited to release four new test environments for benchmarking LLMs‚Äô ability to effectively use tools to accomplish tasks. We hope this makes it easier for everyone to test different LLM and prompting strategies to show what enables the best agentic behavior.Example successful tool use for the Relational Data taskWe designed these tasks to test capabilities we consider to be prerequisites for common agentic workflows, such as planning / task decomposition, function calling, and the ability to override pre-trained biases when needed. If an LLM is unable to solve these types of tasks (without explicit fine-tuning), it will likely struggle to perform and generalize reliably for other workflows where ‚Äúreasoning‚Äù is required. Below are some key take-ways for those eager to see our findings:Overall performance across all tasks (weighted average). Error bars computed using standard error.üí°Key Findings - All of the models can fail over longer trajectories, even for simple tasks.-', metadata={'source': 'https://blog.langchain.dev/benchmarking-agent-tool-use/', 'loc': 'https://blog.langchain.dev/benchmarking-agent-tool-use/', 'lastmod': '2023-12-20T17:54:03.000Z'}),\n",
              "  Document(page_content=\"there are some downsides/dangers:With agents, they can occasionally spiral out of control. That's why we've added controls to our AgentExecutor to cap them at a certain max amount of steps. It's also worth noting that this is a VERY focused agent, in that it's only given one tool (and a pretty simple tool at that). In general, the fewer (and simpler) tools an agent is given, the more likely it is to be reliable.By remembering ai <-> tool interactions, that can hog the context window occasionally. That's why we've included a flag to disable that type of memory, and more generally have made memory pretty plug-and-play.This new agent is in both Python and JS - you can use these guides to get started:JSPythonLLM applications are rapidly evolving. Our NotionQA demo was one of the first we did - and although it was only ~9 months ago the best practices have shifted dramatically since then. This currently represents our best guess at what a GenAI question-answering system should look like, combining the grounded-ness of RAG with the UX of chat and the flexibility of agents.We've got a few more ideas on how this can be further\", metadata={'source': 'https://blog.langchain.dev/conversational-retrieval-agents/', 'loc': 'https://blog.langchain.dev/conversational-retrieval-agents/', 'lastmod': '2023-08-18T21:52:40.000Z'}),\n",
              "  Document(page_content=\"Financial analysis with agentsReimagining the field of traditional financial analysis - from investments to expense tracking - is made possible with agents. Here‚Äôs a few great examples:üìà Can a GPT4-powered AI agent be a good enough performance attribution analyst?In his paper, Bruno Veras De Melo from New York Life Investments explores using LangChain agents to better assess the drivers of an investment portfolio‚Äôs success.Using LangChain as a standard agent framework and prompt engineering techniques like Chain-of-Thought (CoT) and Plan of Solve (PS), he achieves 93% accuracy rates in analyzing performance drivers and 84% accuracy in QA exercises simulating official examination standards.Bruno's chart shows 93% keyword reasoning accuracy from testing few-shot promptsüí∏\\xa0LangGraph expense trackerThis project from Jan Willem Altink (Product Manager @ Esdec) shows how you can use LangGraph to manage your expenses. You can send pictures of invoices (via extraction) ‚Äî the project then lets you structure and categorize expenses and put them in a database.Collaboration & Integrationsü§ùWe üíö\\xa0helping users leverage partner features in the ecosystem by using our\", metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-05-31T16:42:05.000Z'}),\n",
              "  Document(page_content=\"assisted evaluation can help guide you to the most interesting datapoint to look at.üí°Evaluating LLM output using LLMs is NOT perfect, but we think this is currently the best available solution and are bullish on it in the long run.Improved SolutionFinally, we arrive at the exciting part of the blog. Did we manage to improve our solution? And how did we do so?Our final solution is:An agent powered by OpenAIFunctions (OpenAIFunctionsAgent)GPT-4Two tools: a Python REPL and a retrieverA custom prompt with custom instructions on how to think about when to use the Python REPL vs the retrieverThis provides several benefits. First, by giving it access to a Python REPL we give it the ability to do all sorts of queries and analysis. However, as we'll see in some of the comparisons below, the Python REPL can have issues when dealing with text data - in this case the Name column. That is where the retriever can come in handy.üí°Our final solution is an agent with two tools: a Python REPL and a retriever. This allows it to answer questions about the unstructured text, but also perform more traditional data analysis operations.Note that we do\", metadata={'source': 'https://blog.langchain.dev/benchmarking-question-answering-over-csv-data/', 'loc': 'https://blog.langchain.dev/benchmarking-question-answering-over-csv-data/', 'lastmod': '2023-12-05T22:13:01.000Z'})]}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is a good way to evaluate agents?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Langsmith_RAG_{unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "4841f084-6439-414c-8dff-ceaa5792fbdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3eoqBtBQERXP",
        "outputId": "b1c2e49d-7974-4e27-e9ac-ebfb5dfcfe18"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LangSmith is a framework built on the shoulders of LangChain that is designed to track the inner workings of LLMs (Large Language Models) and AI agents within a product. It helps with debugging, testing, monitoring, and evaluating LLM applications. LangSmith can be used independently of LangChain and provides tools to assist users in managing and improving their AI applications.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Create Testing Dataset\n",
        "\n",
        "Now we can create a dataset using some user defined questions, and providing the retrieved context as a \"ground truth\" context.\n",
        "\n",
        "> NOTE: There are many different ways you can approach this specific task - generating ground truth answers with AI, using human experts to generate golden datasets, and more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "test_inputs = [\n",
        "    \"What is LangSmith?\",\n",
        "    \"What is LangServe?\",\n",
        "    \"How could I benchmark RAG on tables?\",\n",
        "    \"What was exciting about LangChain's first birthday?\",\n",
        "    \"What features were released for LangChain on August 7th?\",\n",
        "    \"What is a conversational retrieval agent?\"\n",
        "]\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-v1\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for input in test_inputs:\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : input},\n",
        "      outputs={\"answer\" : base_rag_chain.invoke({\"question\" : input})[\"context\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now we can run the evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CENtd4K_IQa3",
        "outputId": "d6334835-883a-42ed-fa49-29331d33f0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'unique-wire-90' at:\n",
            "https://smith.langchain.com/o/b3011959-9014-5768-9a47-c909b1ca0ccc/datasets/fa434207-2632-42de-813e-cbab8332a052/compare?selectedSessions=24cd7702-a4e1-4356-913c-8efc3a84d36d\n",
            "\n",
            "View all tests for Dataset langsmith-demo-dataset-v1 at:\n",
            "https://smith.langchain.com/o/b3011959-9014-5768-9a47-c909b1ca0ccc/datasets/fa434207-2632-42de-813e-cbab8332a052\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.Contextual Accuracy  feedback.harmfulness error  execution_time                                run_id\n",
            "count                           6.00                  6.00     0            6.00                                     6\n",
            "unique                           NaN                   NaN     0             NaN                                     6\n",
            "top                              NaN                   NaN   NaN             NaN  9a5d793b-4df2-4404-8019-505b25b6c3fb\n",
            "freq                             NaN                   NaN   NaN             NaN                                     1\n",
            "mean                            1.00                  0.00   NaN            1.84                                   NaN\n",
            "std                             0.00                  0.00   NaN            0.32                                   NaN\n",
            "min                             1.00                  0.00   NaN            1.42                                   NaN\n",
            "25%                             1.00                  0.00   NaN            1.58                                   NaN\n",
            "50%                             1.00                  0.00   NaN            1.91                                   NaN\n",
            "75%                             1.00                  0.00   NaN            2.04                                   NaN\n",
            "max                             1.00                  0.00   NaN            2.21                                   NaN\n"
          ]
        }
      ],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "  evaluators=[\n",
        "    RunEvalConfig.CoTQA(llm=eval_llm, prediction_key=\"response\"),\n",
        "    RunEvalConfig.Criteria(\"harmfulness\", prediction_key=\"response\"),\n",
        "  ]\n",
        ")\n",
        "\n",
        "base_rag_base_run = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=base_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOB0u8-RemQ1"
      },
      "source": [
        "## Adding Reranking\n",
        "\n",
        "We'll add reranking to our RAG application to confirm the claim made by [Cohere](https://cohere.com/rerank)!\n",
        "\n",
        "`Improve search performance with a single line of code`\n",
        "\n",
        "We'll put that to the test today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIgP810vPGdi",
        "outputId": "369c5af1-501d-4b03-9fde-336556d637d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Cohere API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Z3VTNGXlO-m2"
      },
      "outputs": [],
      "source": [
        "base_retriever_expander = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\" : 10}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4NMYED37EHm",
        "outputId": "2fa41d26-0695-47bb-c989-dcbfe3057d0c"
      },
      "outputs": [],
      "source": [
        "pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Uk7EPsa3PiUx"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "\n",
        "reranker = CohereRerank()\n",
        "rerank_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker, base_retriever=base_retriever_expander\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBN0h0Zbe7up"
      },
      "source": [
        "### Recreating our Chain with Reranker\n",
        "\n",
        "Now we can recreate our chain using the reranker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kfckjK3QPqhl"
      },
      "outputs": [],
      "source": [
        "rerank_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | rerank_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "rerank_rag_chain = rerank_rag_chain.with_config({\"tags\" : [\"cohere-rerank\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qEHGMLAfISS"
      },
      "source": [
        "### Improved Evaluation\n",
        "\n",
        "Now we can leverage the full suite of LangSmith's evaluation to evaluate our chains on multiple metrics, including custom metrics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nQQXpFg2SV5i"
      },
      "outputs": [],
      "source": [
        "eval_config = RunEvalConfig(\n",
        "  evaluators=[\n",
        "    RunEvalConfig.CoTQA(llm=eval_llm, prediction_key=\"response\"),\n",
        "    RunEvalConfig.Criteria(\"harmfulness\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\n",
        "        {\n",
        "            \"helpfulness\" : (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \"taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        prediction_key=\"response\"\n",
        "    ),\n",
        "    RunEvalConfig.LabeledCriteria(\n",
        "        {\n",
        "            \"litness\" : (\n",
        "                \"Is this submission lit, dope, or cool?\"\n",
        "            )\n",
        "        },\n",
        "        prediction_key=\"response\"\n",
        "    ),\n",
        "    RunEvalConfig.LabeledCriteria(\"conciseness\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\"coherence\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\"relevance\", prediction_key=\"response\")\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPxlhXLmft0A"
      },
      "source": [
        "### Running Eval on Each Chain\n",
        "\n",
        "Now we can evaluate each of our chains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuVnTrncU9LK",
        "outputId": "f908ba03-d307-4d10-ff74-54f82f23ae2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'left-map-62' at:\n",
            "https://smith.langchain.com/o/b3011959-9014-5768-9a47-c909b1ca0ccc/datasets/fa434207-2632-42de-813e-cbab8332a052/compare?selectedSessions=5eb7f6b0-3815-4fd8-b10d-7557662ae28a\n",
            "\n",
            "View all tests for Dataset langsmith-demo-dataset-v1 at:\n",
            "https://smith.langchain.com/o/b3011959-9014-5768-9a47-c909b1ca0ccc/datasets/fa434207-2632-42de-813e-cbab8332a052\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.Contextual Accuracy  feedback.harmfulness  feedback.helpfulness  feedback.litness  feedback.conciseness  feedback.coherence  feedback.relevance error  execution_time                                run_id\n",
            "count                           6.00                  6.00                  5.00              3.00                  3.00                3.00                3.00     0            6.00                                     6\n",
            "unique                           NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN     0             NaN                                     6\n",
            "top                              NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN   NaN             NaN  cdc97fcd-bb6d-46c2-82fe-e30b90a094ef\n",
            "freq                             NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN   NaN             NaN                                     1\n",
            "mean                            1.00                  0.00                  0.60              0.67                  0.67                1.00                0.67   NaN            1.91                                   NaN\n",
            "std                             0.00                  0.00                  0.55              0.58                  0.58                0.00                0.58   NaN            0.45                                   NaN\n",
            "min                             1.00                  0.00                  0.00              0.00                  0.00                1.00                0.00   NaN            1.45                                   NaN\n",
            "25%                             1.00                  0.00                  0.00              0.50                  0.50                1.00                0.50   NaN            1.53                                   NaN\n",
            "50%                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            1.88                                   NaN\n",
            "75%                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            2.19                                   NaN\n",
            "max                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            2.56                                   NaN\n"
          ]
        }
      ],
      "source": [
        "base_chain_results = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=base_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS0m4cunQ1m1",
        "outputId": "c925df4e-a3b7-4ae8-b365-4dd063057491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'long-science-94' at:\n",
            "https://smith.langchain.com/o/b3011959-9014-5768-9a47-c909b1ca0ccc/datasets/fa434207-2632-42de-813e-cbab8332a052/compare?selectedSessions=ab551064-fb88-4406-9e4f-f36067029a99\n",
            "\n",
            "View all tests for Dataset langsmith-demo-dataset-v1 at:\n",
            "https://smith.langchain.com/o/b3011959-9014-5768-9a47-c909b1ca0ccc/datasets/fa434207-2632-42de-813e-cbab8332a052\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.Contextual Accuracy  feedback.harmfulness  feedback.helpfulness  feedback.litness  feedback.conciseness  feedback.coherence  feedback.relevance error  execution_time                                run_id\n",
            "count                           6.00                  6.00                  6.00              5.00                  3.00                3.00                3.00     0            6.00                                     6\n",
            "unique                           NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN     0             NaN                                     6\n",
            "top                              NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN   NaN             NaN  b59d7a26-e690-4bc0-b474-cb921d2da8b4\n",
            "freq                             NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN   NaN             NaN                                     1\n",
            "mean                            0.83                  0.00                  0.83              0.80                  1.00                1.00                0.67   NaN            2.21                                   NaN\n",
            "std                             0.41                  0.00                  0.41              0.45                  0.00                0.00                0.58   NaN            0.56                                   NaN\n",
            "min                             0.00                  0.00                  0.00              0.00                  1.00                1.00                0.00   NaN            1.58                                   NaN\n",
            "25%                             1.00                  0.00                  1.00              1.00                  1.00                1.00                0.50   NaN            1.88                                   NaN\n",
            "50%                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            2.02                                   NaN\n",
            "75%                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            2.59                                   NaN\n",
            "max                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            3.00                                   NaN\n"
          ]
        }
      ],
      "source": [
        "rerank_chain_results = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=rerank_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
